{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook processes and organizes Reddit submissions and comments from the WallStreetBets subreddit into structured datasets for analysis. It includes the following steps:\n",
    "\n",
    "1. Filter and Clean Raw Data:\n",
    "\n",
    "    Filters submissions and comments by date range (2021â€“2023).\n",
    "    Extracts relevant fields and saves them into separate CSV files.\n",
    "\n",
    "2. Split Data by Year:\n",
    "\n",
    "    Divides filtered submissions and comments into separate yearly datasets for easier processing.\n",
    "\n",
    "3. Merge Submissions and Comments:\n",
    "\n",
    "    Associates comments with their corresponding parent submissions.\n",
    "    Separates unmatched comments for review and logs errors and dropped rows.\n",
    "\n",
    "4. Output Flattened Data:\n",
    "\n",
    "    Generates flat, year-specific datasets that combine submissions and comments for analysis.\n",
    "    Logs errors and dropped rows to separate CSV files for transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit Comment Filter and Export\n",
    "This script filters Reddit comments from a JSON file based on a date range and exports the results to a structured CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "from zoneinfo import ZoneInfo  # Use pytz if on Python <3.9\n",
    "\n",
    "# Set your parameters\n",
    "input_file = r\"D:\\reddit\\subreddits23\\wallstreetbets_comments\\wallstreetbets_comments\"\n",
    "output_file = r\"D:\\reddit\\output\\filtered_comments.csv\"\n",
    "from_date = datetime(2021, 1, 1, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "to_date = datetime(2023, 12, 31, 23, 59, 59, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = os.path.dirname(output_file)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def process_comments(input_file, output_file, from_date, to_date):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file_in, open(output_file, 'w', encoding='utf-8', newline='') as file_out:\n",
    "        writer = csv.writer(file_out, quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # Writing CSV header for the comments dataset\n",
    "        writer.writerow([\"id\", \"score\", \"date\", \"time\", \"author\", \"parent_id\", \"link_id\", \"body\"])\n",
    "\n",
    "        for line in file_in:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())  # Load each line as a JSON object\n",
    "                \n",
    "                # Convert created_utc to a timezone-aware datetime in UTC\n",
    "                created_utc = datetime.fromtimestamp(int(obj['created_utc']), tz=timezone.utc)\n",
    "                \n",
    "                # Convert UTC datetime to Eastern Time (ET)\n",
    "                created_et = created_utc.astimezone(ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "                # Filter by date range in ET\n",
    "                if from_date <= created_et <= to_date:\n",
    "                    # Extract fields for the comments dataset\n",
    "                    comment_id = obj.get('id', '')\n",
    "                    score = obj.get('score', '')\n",
    "                    date = created_et.strftime(\"%Y-%m-%d\")\n",
    "                    time = created_et.strftime(\"%H:%M:%S\")  # Exact time in seconds\n",
    "                    author = obj.get('author', '')\n",
    "                    parent_id = obj.get('parent_id', '')\n",
    "                    # Remove the 't3_' prefix from link_id for later matching\n",
    "                    link_id = obj.get('link_id', '').replace('t3_', '')\n",
    "                    body = obj.get('body', '')\n",
    "\n",
    "                    # Write to CSV\n",
    "                    writer.writerow([comment_id, score, date, time, author, parent_id, link_id, body])\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line due to JSON decode error: {e}\")\n",
    "\n",
    "# Run the processing function\n",
    "process_comments(input_file, output_file, from_date, to_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script reads a large CSV file of filtered Reddit comments in chunks, splits the data by year (2021, 2022, 2023), and writes each year's data to a separate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split by year into separate files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths and create separate CSV writers for each year\n",
    "input_file = 'D:/reddit/output/filtered_comments.csv'\n",
    "output_files = {\n",
    "    2021: 'D:/reddit/output/filtered_comments_2021.csv',\n",
    "    2022: 'D:/reddit/output/filtered_comments_2022.csv',\n",
    "    2023: 'D:/reddit/output/filtered_comments_2023.csv'\n",
    "}\n",
    "\n",
    "# Initialize files for each year and write headers with full columns\n",
    "for year, file_path in output_files.items():\n",
    "    with open(file_path, 'w', encoding='utf-8', newline='') as file:\n",
    "        file.write(\"id,score,date,time,author,parent_id,link_id,body\\n\")  # Writing header\n",
    "\n",
    "# Process the input file in chunks\n",
    "chunk_size = 100000  # Adjust the chunk size based on available memory\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, parse_dates=[\"date\"]):\n",
    "    # Drop rows with invalid dates (if any)\n",
    "    chunk = chunk.dropna(subset=['date'])\n",
    "\n",
    "    # Ensure columns are in the correct order to avoid misalignment\n",
    "    chunk = chunk[['id', 'score', 'date', 'time', 'author', 'parent_id', 'link_id', 'body']]\n",
    "\n",
    "    # Extract the year from the 'date' column\n",
    "    chunk['year'] = chunk['date'].dt.year\n",
    "\n",
    "    # Filter each chunk by year and append to the respective output file\n",
    "    for year in [2021, 2022, 2023]:\n",
    "        df_year = chunk[chunk['year'] == year]\n",
    "        if not df_year.empty:\n",
    "            with open(output_files[year], 'a', encoding='utf-8', newline='') as file:\n",
    "                df_year.to_csv(file, columns=['id', 'score', 'date', 'time', 'author', 'parent_id', 'link_id', 'body'], header=False, index=False)\n",
    "\n",
    "print(\"Data successfully split by year into separate files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Reddit Submissions by Date\n",
    "This script processes a JSON file of Reddit submissions, filters entries based on a specified date range, and saves the filtered submissions to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "from zoneinfo import ZoneInfo  # Use pytz if on Python <3.9\n",
    "\n",
    "# Set your parameters\n",
    "input_file = r\"D:\\reddit\\subreddits23\\wallstreetbets_submissions\\wallstreetbets_submissions\"\n",
    "output_file = r\"D:\\reddit\\output\\filtered_submissions.csv\"\n",
    "from_date = datetime(2021, 1, 1, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "to_date = datetime(2023, 12, 31, 23, 59, 59, tzinfo=ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = os.path.dirname(output_file)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def process_file(input_file, output_file, from_date, to_date):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file_in, open(output_file, 'w', encoding='utf-8', newline='') as file_out:\n",
    "        writer = csv.writer(file_out, quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # Writing CSV header based on required fields\n",
    "        writer.writerow([\"id\", \"score\", \"date\", \"time\", \"title\", \"author\", \"permalink\", \"selftext\", \"url\"])\n",
    "\n",
    "        for line in file_in:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())  # Load each line as a JSON object\n",
    "                \n",
    "                # Convert created_utc to a timezone-aware datetime in UTC\n",
    "                created_utc = datetime.fromtimestamp(int(obj['created_utc']), tz=timezone.utc)\n",
    "                \n",
    "                # Convert UTC datetime to Eastern Time (ET)\n",
    "                created_et = created_utc.astimezone(ZoneInfo(\"America/New_York\"))\n",
    "\n",
    "                # Filter by date range in ET\n",
    "                if from_date <= created_et <= to_date:\n",
    "                    # Extract fields based on requirements\n",
    "                    submission_id = obj.get('id', '')\n",
    "                    score = obj.get('score', '')  # Assuming 'score' is available; adjust if needed\n",
    "                    date = created_et.strftime(\"%Y-%m-%d\")\n",
    "                    time = created_et.strftime(\"%H:%M:%S\")  # Exact time in seconds\n",
    "                    title = obj.get('title', '')\n",
    "                    author = obj.get('author', '')\n",
    "                    permalink = obj.get('permalink', '')\n",
    "                    selftext = obj.get('selftext', '')\n",
    "                    url = obj.get('url', '')\n",
    "\n",
    "                    # Write to CSV\n",
    "                    writer.writerow([submission_id, score, date, time, title, author, permalink, selftext, url])\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line due to JSON decode error: {e}\")\n",
    "\n",
    "# Run the processing function\n",
    "process_file(input_file, output_file, from_date, to_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script splits a large CSV file of filtered Reddit submissions into separate files by year (2021, 2022, 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissions data successfully split by year into separate files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths and create separate CSV writers for each year\n",
    "input_file = 'D:/reddit/output/filtered_submissions.csv'\n",
    "output_files = {\n",
    "    2021: 'D:/reddit/output/filtered_submissions_2021.csv',\n",
    "    2022: 'D:/reddit/output/filtered_submissions_2022.csv',\n",
    "    2023: 'D:/reddit/output/filtered_submissions_2023.csv'\n",
    "}\n",
    "\n",
    "# Initialize files for each year and write headers with full columns\n",
    "for year, file_path in output_files.items():\n",
    "    with open(file_path, 'w', encoding='utf-8', newline='') as file:\n",
    "        file.write(\"id,score,date,time,title,author,permalink,selftext,url\\n\")  # Writing header\n",
    "\n",
    "# Process the input file in chunks\n",
    "chunk_size = 100000  # Adjust the chunk size based on available memory\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, parse_dates=[\"date\"]):\n",
    "    # Drop rows with invalid dates (if any)\n",
    "    chunk = chunk.dropna(subset=['date'])\n",
    "\n",
    "    # Ensure columns are in the correct order to avoid misalignment\n",
    "    chunk = chunk[['id', 'score', 'date', 'time', 'title', 'author', 'permalink', 'selftext', 'url']]\n",
    "\n",
    "    # Extract the year from the 'date' column\n",
    "    chunk['year'] = chunk['date'].dt.year\n",
    "\n",
    "    # Filter each chunk by year and append to the respective output file\n",
    "    for year in [2021, 2022, 2023]:\n",
    "        df_year = chunk[chunk['year'] == year]\n",
    "        if not df_year.empty:\n",
    "            with open(output_files[year], 'a', encoding='utf-8', newline='') as file:\n",
    "                df_year.to_csv(file, columns=['id', 'score', 'date', 'time', 'title', 'author', 'permalink', 'selftext', 'url'], header=False, index=False)\n",
    "\n",
    "print(\"Submissions data successfully split by year into separate files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Reddit Submissions and Comments by Year\n",
    "This script processes and merges Reddit submissions and comments for each year (2021, 2022, 2023). It associates comments with their parent submissions, logs unmatched comments, and outputs a flattened dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_67176\\438859393.py:58: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  comments_df = pd.read_csv(comments_files[year], on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 9 rows with NaN parent_id for year 2021\n",
      "Unmatched comments for 2021 saved to D:/reddit/output/unmatched_comments_2021.csv\n",
      "Flattened data for 2021 saved to D:/reddit/output/flattened_submissions_with_comments_2021.csv\n",
      "Dropped 0 rows with NaN parent_id for year 2022\n",
      "Unmatched comments for 2022 saved to D:/reddit/output/unmatched_comments_2022.csv\n",
      "Flattened data for 2022 saved to D:/reddit/output/flattened_submissions_with_comments_2022.csv\n",
      "Dropped 0 rows with NaN parent_id for year 2023\n",
      "Unmatched comments for 2023 saved to D:/reddit/output/unmatched_comments_2023.csv\n",
      "Flattened data for 2023 saved to D:/reddit/output/flattened_submissions_with_comments_2023.csv\n",
      "No errors encountered during processing.\n",
      "Drop log saved to D:/reddit/output/drop_log.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Define file paths for submissions and comments by year\n",
    "submissions_files = {\n",
    "    2021: 'D:/reddit/output/filtered_submissions_2021.csv',\n",
    "    2022: 'D:/reddit/output/filtered_submissions_2022.csv',\n",
    "    2023: 'D:/reddit/output/filtered_submissions_2023.csv'\n",
    "}\n",
    "comments_files = {\n",
    "    2021: 'D:/reddit/output/filtered_comments_2021.csv',\n",
    "    2022: 'D:/reddit/output/filtered_comments_2022.csv',\n",
    "    2023: 'D:/reddit/output/filtered_comments_2023.csv'\n",
    "}\n",
    "\n",
    "# Output files for merged data and unmatched comments\n",
    "output_files = {\n",
    "    2021: 'D:/reddit/output/flattened_submissions_with_comments_2021.csv',\n",
    "    2022: 'D:/reddit/output/flattened_submissions_with_comments_2022.csv',\n",
    "    2023: 'D:/reddit/output/flattened_submissions_with_comments_2023.csv'\n",
    "}\n",
    "unmatched_files = {\n",
    "    2021: 'D:/reddit/output/unmatched_comments_2021.csv',\n",
    "    2022: 'D:/reddit/output/unmatched_comments_2022.csv',\n",
    "    2023: 'D:/reddit/output/unmatched_comments_2023.csv'\n",
    "}\n",
    "error_log_file = 'D:/reddit/output/error_log.csv'  # Error log file to keep track of problematic rows\n",
    "\n",
    "# Initialize a list to keep track of errors and dropped rows\n",
    "error_log = []\n",
    "drop_log = []\n",
    "\n",
    "# Process each year separately\n",
    "for year in [2021, 2022, 2023]:\n",
    "    # Load submissions with error handling\n",
    "    try:\n",
    "        submissions_df = pd.read_csv(submissions_files[year], on_bad_lines='skip')\n",
    "    except pd.errors.ParserError as e:\n",
    "        error_log.append({'file': submissions_files[year], 'year': year, 'error': str(e)})\n",
    "        print(f\"Error reading submissions file for {year}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Rename columns in submissions to differentiate them when merged with comments\n",
    "    submissions_df = submissions_df.rename(columns={\n",
    "        \"id\": \"submission_id\",\n",
    "        \"score\": \"submission_score\",\n",
    "        \"date\": \"submission_date\",\n",
    "        \"time\": \"submission_time\",\n",
    "        \"title\": \"submission_title\",\n",
    "        \"author\": \"submission_author\",\n",
    "        \"permalink\": \"submission_permalink\",\n",
    "        \"selftext\": \"submission_selftext\",\n",
    "        \"url\": \"submission_url\"\n",
    "    })\n",
    "\n",
    "    # Load comments with error handling\n",
    "    try:\n",
    "        comments_df = pd.read_csv(comments_files[year], on_bad_lines='skip')\n",
    "    except pd.errors.ParserError as e:\n",
    "        error_log.append({'file': comments_files[year], 'year': year, 'error': str(e)})\n",
    "        print(f\"Error reading comments file for {year}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Count the original number of rows in comments\n",
    "    original_row_count = len(comments_df)\n",
    "\n",
    "    # Remove rows with NaN values in `parent_id`\n",
    "    comments_df = comments_df.dropna(subset=['parent_id'])\n",
    "\n",
    "    # Count and log the number of dropped rows\n",
    "    dropped_row_count = original_row_count - len(comments_df)\n",
    "    drop_log.append({'year': year, 'dropped_rows': dropped_row_count})\n",
    "    print(f\"Dropped {dropped_row_count} rows with NaN parent_id for year {year}\")\n",
    "\n",
    "    # Filter comments to include only those with `parent_id` pointing to a submission (i.e., starts with \"t3_\")\n",
    "    comments_df = comments_df[comments_df['parent_id'].str.startswith('t3_')].copy()\n",
    "\n",
    "    # Clean up `parent_id` by removing the \"t3_\" prefix\n",
    "    comments_df['parent_id'] = comments_df['parent_id'].str.replace('t3_', '')\n",
    "\n",
    "    # Rename columns in comments for clarity\n",
    "    comments_df = comments_df.rename(columns={\n",
    "        \"id\": \"comment_id\",\n",
    "        \"score\": \"comment_score\",\n",
    "        \"date\": \"comment_date\",\n",
    "        \"time\": \"comment_time\",\n",
    "        \"author\": \"comment_author\",\n",
    "        \"body\": \"comment_body\"\n",
    "    })\n",
    "\n",
    "    # Merge comments with their associated submission data (left join to keep all comments)\n",
    "    merged_df = comments_df.merge(submissions_df, left_on='parent_id', right_on='submission_id', how='left')\n",
    "\n",
    "    # Separate unmatched comments (those with NaN in `submission_id` column)\n",
    "    unmatched_comments_df = merged_df[merged_df['submission_id'].isna()]\n",
    "\n",
    "    # Log unmatched comments to a separate CSV file\n",
    "    if not unmatched_comments_df.empty:\n",
    "        unmatched_comments_df.to_csv(unmatched_files[year], index=False)\n",
    "        print(f\"Unmatched comments for {year} saved to {unmatched_files[year]}\")\n",
    "    else:\n",
    "        print(f\"No unmatched comments for {year}.\")\n",
    "\n",
    "    # Filter out unmatched comments from the merged data (optional)\n",
    "    matched_comments_df = merged_df.dropna(subset=['submission_id'])\n",
    "\n",
    "    # Save the matched data to a flat CSV file\n",
    "    matched_comments_df.to_csv(output_files[year], index=False)\n",
    "    print(f\"Flattened data for {year} saved to {output_files[year]}\")\n",
    "\n",
    "# Save the error log to a CSV file if there were any errors\n",
    "if error_log:\n",
    "    pd.DataFrame(error_log).to_csv(error_log_file, index=False)\n",
    "    print(f\"Error log saved to {error_log_file}\")\n",
    "else:\n",
    "    print(\"No errors encountered during processing.\")\n",
    "\n",
    "# Save the drop log to a CSV file to record the dropped rows\n",
    "drop_log_file = 'D:/reddit/output/drop_log.csv'\n",
    "pd.DataFrame(drop_log).to_csv(drop_log_file, index=False)\n",
    "print(f\"Drop log saved to {drop_log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
