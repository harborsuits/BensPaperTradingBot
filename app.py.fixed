import streamlit as st

# Set page configuration - MUST be the first Streamlit command
st.set_page_config(
    page_title="Trading Strategy Dashboard",
    page_icon="📈",
    layout="wide",
    initial_sidebar_state="expanded"
)

import pandas as pd
import numpy as np
try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    st.warning("Plotly is not installed. Some visualizations may not be available.")
from datetime import datetime, timedelta, timezone
import os
import uuid
import json

# Import modular strategy components
try:
    # Try to import full modular strategy components
    from trading_bot.strategies.modular_strategy_system import ComponentType, MarketCondition
    from trading_bot.strategies.components.component_registry import get_component_registry
    from trading_bot.strategies.modular_strategy_integration import ModularStrategyFactory
    from trading_bot.ui.modular_strategy_ui import ModularStrategyUI
    from trading_bot.strategies.optimizer.component_optimizer import GridSearchOptimizer, RandomSearchOptimizer
    from trading_bot.strategies.marketplace.component_marketplace import ComponentMarketplace
    from trading_bot.strategies.analytics.component_analytics import ComponentAnalytics
    modular_strategy_full = True
except ImportError as e:
    modular_strategy_full = False
    logger.warning(f"Full modular strategy components not available: {e}")

# Try simplified UI if full component system is not available
try:
    from trading_bot.ui.modular_strategy_simplified import ModularStrategySimplifiedUI
    modular_strategy_simplified = True
except ImportError as e:
    modular_strategy_simplified = False
    logger.warning(f"Simplified modular strategy UI not available: {e}")

# Import marketplace UI components
try:
    from trading_bot.ui.marketplace_ui import MarketplaceUI
    from trading_bot.marketplace.api.marketplace_api import MarketplaceAPI
    from trading_bot.marketplace.api.security import SecurityManager
    marketplace_ui_available = True
    logger.info("Component marketplace UI loaded successfully")
except ImportError as e:
    marketplace_ui_available = False
    logger.warning(f"Component marketplace UI not available: {e}")
    
# Import trading launcher UI
try:
    from trading_bot.ui.trading_launcher import TradingLauncher
    trading_launcher_available = True
    logger.info("Trading launcher UI loaded successfully")
except ImportError as e:
    trading_launcher_available = False
    logger.warning(f"Trading launcher UI not available: {e}")
    
# Set overall availability flag    
modular_strategy_available = modular_strategy_full or modular_strategy_simplified
import sys
import logging
import json
# Commented out socketio - not needed for Streamlit app
import re
from collections import defaultdict
import concurrent.futures
import threading
import time
from functools import lru_cache
import random
import math
# NEW: For chat assistant
import uuid
from typing import List, Dict, Any, Tuple

# Add parent directory to path to support imports
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("dashboard")

# Try to import LiveDataManager for real-time features
try:
    from live_data_manager import LiveDataManager
    LIVE_DATA_MANAGER_AVAILABLE = True
except ImportError:
    LIVE_DATA_MANAGER_AVAILABLE = False
    logger.warning("LiveDataManager not available, live data features will be disabled")

# Define risk multipliers for different risk levels
risk_multipliers = {
    "Low": 0.5,      # 50% of base position size
    "Moderate": 1.0, # 100% of base position size (neutral)
    "High": 1.5,     # 150% of base position size
    "Aggressive": 2.0 # 200% of base position size
}

# Default risk level
risk_level = "Moderate"

# Default selected strategies
selected_strategies = ["Momentum", "Mean Reversion", "Trend Following", "Volatility Breakout"]

# Import trading bot components
try:
    from trading_bot.portfolio_state import PortfolioStateManager
    from trading_bot.data.market_data_provider import create_data_provider
    from trading_bot.strategies.momentum import MomentumStrategy
    from trading_bot.strategies.mean_reversion import MeanReversionStrategy
    from trading_bot.strategies.trend_following import TrendFollowingStrategy
    try:
        from trading_bot.strategies.volatility_breakout import VolatilityBreakout
        volatility_breakout_available = True
    except ImportError as e:
        logger.warning(f"Error importing VolatilityBreakout: {e}")
        volatility_breakout_available = False
    from trading_bot.realtime.real_time_data_connector import RealTimeDataConnector
    from trading_bot.realtime.message_queue import MessageBroker
    try:
        from trading_bot.risk.psychological_risk import PsychologicalRiskManager
        psychological_risk_available = True
    except ImportError as e:
        logger.warning(f"Error importing PsychologicalRiskManager: {e}")
        psychological_risk_available = False
    # Import the optimizer components
    try:
        from trading_bot.ui.optimizer_ui import OptimizerUI
        from trading_bot.ml_pipeline.optimizer_integration import OptimizerIntegration
        from trading_bot.strategies.hybrid_strategy_optimizer import HybridStrategyOptimizer
        optimizer_available = True
        logger.info("Strategy optimizer components loaded successfully")
    except ImportError as e:
        logger.warning(f"Error importing optimizer components: {e}")
        optimizer_available = False
        
    # Import LLM integration components
    try:
        from trading_bot.llm_integration.integration import LLMTradingIntegration
        from trading_bot.llm_integration.financial_llm_engine import LLMProvider
        from trading_bot.llm_integration.memory_system import MemoryType
        from trading_bot.llm_integration.reasoning_engine import ReasoningTask
        llm_integration_available = True
        logger.info("LLM integration components loaded successfully")
    except ImportError as e:
        logger.warning(f"Error importing LLM integration components: {e}")
        llm_integration_available = False
    
    # Import ML regime detector and other integration components
    try:
        from trading_bot.ml_pipeline.ml_regime_detector import MLRegimeDetector
        from trading_bot.ml_pipeline.portfolio_optimizer import PortfolioOptimizer
        from trading_bot.triggers.regime_change_notifier import RegimeChangeNotifier
        from trading_bot.automated_trader import AutomatedTrader
        integration_components_available = True
        logger.info("ML regime detector and integration components loaded successfully")
    except ImportError as e:
        logger.warning(f"Error importing integration components: {e}")
        integration_components_available = False
        
    # Import event-driven architecture components
    try:
        from trading_bot.event_system import (
            EventManager, EventBus, MessageQueue, ChannelManager,
            QueueType, QueueBackend, Event, EventType
        )
        event_system_available = True
        logger.info("Event-driven architecture components loaded successfully")
    except ImportError as e:
        logger.warning(f"Error importing event-driven architecture components: {e}")
        event_system_available = False
    
    # Import trading mode system components
    try:
        from trading_bot.trading_modes import (
            BaseTradingMode, StandardTradingMode, Order, OrderType
        )
        trading_mode_available = True
        logger.info("Trading mode system components loaded successfully")
    except ImportError as e:
        logger.warning(f"Error importing trading mode components: {e}")
        trading_mode_available = False

    # Set flag for using real components
    USING_REAL_COMPONENTS = True
    logger.info("Successfully imported trading_bot components")
except ImportError as e:
    # If imports fail, we'll use mock data
    logger.warning(f"Error importing trading_bot components: {e}")
    logger.warning("Using mock data for dashboard")
    USING_REAL_COMPONENTS = False
    volatility_breakout_available = False
    psychological_risk_available = False

# Initialize API keys for external services
# Replace hardcoded API keys with environment variables
ALPACA_API_KEY = "6165f902-b7a3-408c-9512-4e554225d825"
FINNHUB_API_KEY = "pcIfIzF_AiAd2Ps0ifLTXRtuA2BbBVtS"
MARKETAUX_API_KEY = "7PgROm6BE4m6ejBW8unmZnnYS6kIygu5lwzpfd9K"
NEWSDATA_API_KEY = "pub_81036c20e73907398317875951d4569722f2a"
GNEWS_API_KEY = "00c755186577632fbf651fc38e39858b"
MEDIASTACK_API_KEY = "3ff958493e0f1d8cf9af5e8425c8f5a3"  # 100 calls/month
CURRENTS_API_KEY = "O5_JjrWdlLN2v93iuKbhEhA9OSIYfChf4Cx9XE9xXgW1oYTC"  # 600 calls/month
NYTIMES_API_KEY = "NosApZGLGvPusEz30Fk4lQban19z9PTo"  # 4000 calls/day

# Initialize Alpaca API client if requests is available
try:
    import requests
    import json
    import pandas as pd
    from datetime import datetime, timedelta
    
    # Initialize Alpaca client
    class AlpacaClient:
        def __init__(self, api_key):
            self.api_key = api_key
            self.base_url = "https://paper-api.alpaca.markets"  # Using paper trading endpoint
            self.data_url = "https://data.alpaca.markets"
            self.headers = {
                "APCA-API-KEY-ID": api_key,
                "Accept": "application/json"
            }
        
        def get_account(self):
            """Get account information"""
            try:
                response = requests.get(f"{self.base_url}/v2/account", headers=self.headers)
                if response.status_code == 200:
                    return response.json()
                else:
                    logger.warning(f"Alpaca API account request failed: {response.status_code}, {response.text}")
                    return None
            except Exception as e:
                logger.error(f"Error getting Alpaca account info: {e}")
                return None
        
        def get_positions(self):
            """Get current positions"""
            try:
                response = requests.get(f"{self.base_url}/v2/positions", headers=self.headers)
                if response.status_code == 200:
                    return response.json()
                else:
                    logger.warning(f"Alpaca API positions request failed: {response.status_code}, {response.text}")
                    return []
            except Exception as e:
                logger.error(f"Error getting Alpaca positions: {e}")
                return []
        
        def get_bars(self, symbols, timeframe="1Day", limit=100):
            """Get historical bar data for symbols"""
            if isinstance(symbols, str):
                symbols = [symbols]
                
            try:
                # Format symbols for the API
                symbols_param = ",".join(symbols)
                
                # Set up parameters
                params = {
                    "symbols": symbols_param,
                    "timeframe": timeframe,
                    "limit": limit
                }
                
                # Make the request
                response = requests.get(
                    f"{self.data_url}/v2/stocks/bars", 
                    headers=self.headers,
                    params=params
                )
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Process and convert to pandas DataFrames
                    result = {}
                    if "bars" in data:
                        for symbol, bars in data["bars"].items():
                            if bars:
                                df = pd.DataFrame(bars)
                                # Rename columns to match our expected format
                                df = df.rename(columns={
                                    "t": "timestamp",
                                    "o": "open",
                                    "h": "high",
                                    "l": "low",
                                    "c": "close",
                                    "v": "volume"
                                })
                                # Convert timestamp to datetime
                                df["timestamp"] = pd.to_datetime(df["timestamp"])
                                # Set timestamp as index
                                df = df.set_index("timestamp")
                                result[symbol] = df
                    
                    return result
                else:
                    logger.warning(f"Alpaca API bars request failed: {response.status_code}, {response.text}")
                    return {}
            except Exception as e:
                logger.error(f"Error getting Alpaca bars: {e}")
                return {}
    
    # Initialize FinnHub client
    class FinnhubClient:
        def __init__(self, api_key):
            self.api_key = api_key
            self.base_url = "https://finnhub.io/api/v1"
        
        def get_quote(self, symbol):
            """Get current quote for a symbol"""
            try:
                response = requests.get(
                    f"{self.base_url}/quote",
                    params={"symbol": symbol, "token": self.api_key}
                )
                
                if response.status_code == 200:
                    return response.json()
                else:
                    logger.warning(f"Finnhub API quote request failed: {response.status_code}, {response.text}")
                    return None
            except Exception as e:
                logger.error(f"Error getting Finnhub quote: {e}")
                return None
        
        def get_company_news(self, symbol, from_date, to_date):
            """Get company news for a symbol"""
            try:
                response = requests.get(
                    f"{self.base_url}/company-news",
                    params={
                        "symbol": symbol,
                        "from": from_date,
                        "to": to_date,
                        "token": self.api_key
                    }
                )
                
                if response.status_code == 200:
                    return response.json()
                else:
                    logger.warning(f"Finnhub API news request failed: {response.status_code}, {response.text}")
                    return []
            except Exception as e:
                logger.error(f"Error getting Finnhub news: {e}")
                return []
        
        def get_market_news(self):
            """Get general market news"""
            try:
                response = requests.get(
                    f"{self.base_url}/news",
                    params={
                        "category": "general",
                        "token": self.api_key
                    }
                )
                
                if response.status_code == 200:
                    return response.json()
                else:
                    logger.warning(f"Finnhub API market news request failed: {response.status_code}, {response.text}")
                    return []
            except Exception as e:
                logger.error(f"Error getting Finnhub market news: {e}")
                return []
        
        def get_company_profile(self, symbol):
            """Get company profile information"""
            try:
                response = requests.get(
                    f"{self.base_url}/stock/profile2",
                    params={"symbol": symbol, "token": self.api_key}
                )
                
                if response.status_code == 200:
                    return response.json()
                else:
                    logger.warning(f"Finnhub API profile request failed: {response.status_code}, {response.text}")
                    return None
            except Exception as e:
                logger.error(f"Error getting Finnhub company profile: {e}")
                return None
    
    # Initialize the clients
    try:
        alpaca_client = AlpacaClient(ALPACA_API_KEY)
        finnhub_client = FinnhubClient(FINNHUB_API_KEY)
        logger.info("Successfully initialized Alpaca and Finnhub API clients")
        EXTERNAL_APIS_AVAILABLE = True
    except Exception as e:
        logger.error(f"Error initializing API clients: {e}")
        EXTERNAL_APIS_AVAILABLE = False
except ImportError as e:
    logger.warning(f"Could not import required packages for API integration: {e}")
    EXTERNAL_APIS_AVAILABLE = False

# Initialize LiveDataManager if available
if LIVE_DATA_MANAGER_AVAILABLE:
    live_data_manager = LiveDataManager()
    # Check for auto-refresh
    live_data_manager.check_auto_refresh()

# Initialize global state
if USING_REAL_COMPONENTS:
    # Initialize real portfolio state manager
    portfolio_state = PortfolioStateManager()
    
    # Initialize LLM integration if available
    llm_trading = None
    if 'llm_integration_available' in locals() and llm_integration_available and 'integration_components_available' in locals() and integration_components_available:
        try:
            # Get path to config file
            config_path = os.path.join(current_dir, "config.py")
            
            # Initialize LLM integration
            llm_trading = LLMTradingIntegration(
                config_path=config_path,
                enable_memory=True,
                debug=False
            )
            
            # Initialize integration components
            ml_regime_detector = MLRegimeDetector()
            portfolio_optimizer = PortfolioOptimizer()
            regime_change_notifier = RegimeChangeNotifier()
            automated_trader = AutomatedTrader()
            
            # Connect components
            llm_trading.connect_existing_components(
                ml_regime_detector=ml_regime_detector,
                portfolio_optimizer=portfolio_optimizer,
                regime_change_notifier=regime_change_notifier,
                automated_trader=automated_trader
            )
            
            # Set up memory consolidation task
            def run_memory_consolidation():
                """Run memory consolidation as a background task"""
                while True:
                    # Wait until market is closed (assuming US market hours)
                    current_hour = datetime.now().hour
                    if 4 <= current_hour <= 9:  # 4am to 9am ET (before market open)
                        logger.info("Running memory consolidation...")
                        try:
                            llm_trading.memory_system.consolidate_memories()
                            logger.info("Memory consolidation complete")
                        except Exception as e:
                            logger.error(f"Error during memory consolidation: {e}")
                    # Sleep for 1 hour before checking again
                    time.sleep(3600)
            
            # Start memory consolidation in a background thread
            memory_thread = threading.Thread(
                target=run_memory_consolidation,
                daemon=True  # Thread will exit when main program exits
            )
            memory_thread.start()
            
            logger.info("LLM integration successfully initialized and connected")
        except Exception as e:
            logger.error(f"Error initializing LLM integration: {e}")
            llm_integration_available = False
    
    # Initialize market data provider
    try:
        # Try to create market data provider based on configuration
        config_path = os.path.join(current_dir, "trading_bot/config/data_providers.json")
        data_provider = create_data_provider("alpaca", config_path)
        logger.info("Initialized market data provider")
    except Exception as e:
        logger.error(f"Error initializing market data provider: {e}")
        # Fall back to mock data
        USING_REAL_COMPONENTS = False
        
    # Initialize trading strategies
    if USING_REAL_COMPONENTS:
        strategies = {
            "Momentum": MomentumStrategy(),
            "Mean Reversion": MeanReversionStrategy(),
            "Trend Following": TrendFollowingStrategy(),
        }
        # Add VolatilityBreakout if available
        if volatility_breakout_available:
            strategies["Volatility Breakout"] = VolatilityBreakout()
        logger.info(f"Initialized {len(strategies)} trading strategies")

# Add custom CSS - enhanced with card styles for LLM Insights
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1E88E5;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: 600;
        color: #0D47A1;
        margin-top: 1rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        border-radius: 5px;
        padding: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .highlight {
        color: #2E7D32;
        font-weight: 600;
    }
    .warning {
        color: #C62828;
        font-weight: 600;
    }
    .neutral {
        color: #F57C00;
        font-weight: 600;
    }
</style>
""", unsafe_allow_html=True)

# News API cache system
class NewsCache:
    def __init__(self):
        self.articles = {}
        self.last_updated = {}
        self.api_calls = defaultdict(int)
        self.api_call_dates = defaultdict(list)
        self.api_call_timestamps = defaultdict(list)  # Track timestamps for rate limiting
        self.month_start = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        
        # API limits (removing NYTimes)
        self.daily_limits = {
            "finnhub": 5000,      # 60 calls/minute (safe estimate for daily)
            "marketaux": 100,     # 100 calls/day
            "newsdata": 2000,     # 2000 calls/day
            "gnews": 100,         # Assumed 100 calls/day
            "mediastack": 3,      # 100 calls/month (~3/day)
            "currents": 20        # 600 calls/month (~20/day)
        }
        
        # API rate limits (calls per period in seconds)
        self.rate_limits = {
            "finnhub": {"calls": 60, "period": 60},    # 60 calls per minute
            "marketaux": {"calls": 1, "period": 300},  # 1 call per 5 minutes to stay safe
            "newsdata": {"calls": 8, "period": 3600},  # 8 calls per hour (conservative)
            "gnews": {"calls": 1, "period": 900},      # 1 call per 15 minutes to stay safe
            "mediastack": {"calls": 1, "period": 28800},  # 1 call per 8 hours (monthly limit)
            "currents": {"calls": 1, "period": 4320}     # 1 call per 72 minutes (monthly limit)
        }
        
        # API weights by query type (higher = better)
        self.api_weights = {
            "ticker": {
                "finnhub": 10,     # Best for tickers
                "marketaux": 8,    # Good for tickers
                "newsdata": 5,     # OK for tickers
                "gnews": 3,        # Not ideal for tickers
                "nytimes": 6,      # Good for company news
                "mediastack": 4,   # OK for tickers
                "currents": 4      # OK for tickers
            },
            "market": {
                "finnhub": 6,      # Good for market news
                "marketaux": 9,    # Excellent for market news
                "newsdata": 8,     # Very good for market news
                "gnews": 7,        # Good for market news
                "nytimes": 9,      # Excellent for market news
                "mediastack": 5,   # OK for market news
                "currents": 6      # Good for market news
            },
            "topic": {
                "finnhub": 4,      # OK for topics
                "marketaux": 7,    # Good for topics
                "newsdata": 9,     # Excellent for topics
                "gnews": 8,        # Very good for topics
                "nytimes": 10,     # Best for topics
                "mediastack": 6,   # Good for topics
                "currents": 7      # Very good for topics
            }
        }
    
    def get(self, key, max_age_minutes=60):
        """Get news for a key if it exists and is fresh"""
        if key in self.articles and key in self.last_updated:
            age = (datetime.now() - self.last_updated[key]).total_seconds() / 60
            if age <= max_age_minutes:
                return self.articles[key]
        return None
    
    def update(self, key, articles):
        """Update the cache with new articles"""
        # If we already have articles for this key, merge and deduplicate
        if key in self.articles:
            existing_urls = {article.get('url') for article in self.articles[key]}
            new_articles = [article for article in articles if article.get('url') not in existing_urls]
            
            # Combine and sort by timestamp (newest first)
            combined = self.articles[key] + new_articles
            try:
                combined.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
            except:
                # If sorting fails, just append new articles
                combined = self.articles[key] + new_articles
                
            # Update with combined list
            self.articles[key] = combined
        else:
            # Just store the new articles
            self.articles[key] = articles
            
        # Update the timestamp
        self.last_updated[key] = datetime.now()
    
    def log_api_call(self, provider):
        """Log an API call to monitor usage and enforce rate limits"""
        # Check if we've entered a new month and reset monthly counters if needed
        now = datetime.now()
        if now.month != self.month_start.month or now.year != self.month_start.year:
            # Reset monthly counters for monthly-limited APIs
            if provider in ["mediastack", "currents"]:
                self.api_calls[provider] = 0
                self.api_call_dates[provider] = []
                self.api_call_timestamps[provider] = []
            self.month_start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        
        today = now.date()
        
        # Record the call
        self.api_calls[provider] += 1
        self.api_call_dates[provider].append(today)
        self.api_call_timestamps[provider].append(now)
        
        # Clean up old dates (keep only last 30 days)
        cutoff_date = today - timedelta(days=30)
        self.api_call_dates[provider] = [d for d in self.api_call_dates[provider] if d >= cutoff_date]
        
        # Clean up old timestamps (keep only last 24 hours)
        cutoff_time = now - timedelta(hours=24)
        self.api_call_timestamps[provider] = [t for t in self.api_call_timestamps[provider] if t >= cutoff_time]
        
        # Log the call with different format for monthly APIs
        if provider in ["mediastack", "currents"]:
            monthly_calls = self.get_calls_this_month(provider)
            logger.info(f"API call to {provider}. Total this month: {monthly_calls}")
        else:
            logger.info(f"API call to {provider}. Total today: {self.get_calls_today(provider)}")
    
    def get_calls_today(self, provider):
        """Get number of calls made today for a provider"""
        today = datetime.now().date()
        return sum(1 for d in self.api_call_dates.get(provider, []) if d == today)
    
    def get_calls_this_month(self, provider):
        """Get number of calls made this month for a provider"""
        # For APIs with monthly limits
        month_start = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        return sum(1 for t in self.api_call_timestamps.get(provider, []) if t >= month_start)
    
    def get_all_calls_today(self):
        """Get all API calls made today across providers"""
        result = {}
        today = datetime.now().date()
        for provider in self.api_call_dates:
            # For monthly APIs, show monthly stats
            if provider in ["mediastack", "currents"]:
                result[provider] = self.get_calls_this_month(provider)
            else:
                result[provider] = sum(1 for d in self.api_call_dates[provider] if d == today)
        return result
    
    def is_rate_limited(self, provider):
        """Check if we're currently rate limited for this provider"""
        if provider not in self.rate_limits:
            return False
            
        # Get calls within the rate limit period
        now = datetime.now()
        period_seconds = self.rate_limits[provider]["period"]
        period_start = now - timedelta(seconds=period_seconds)
        
        # Count calls in this period
        calls_in_period = sum(1 for t in self.api_call_timestamps.get(provider, []) 
                             if t >= period_start)
        
        # Check if we've hit the limit
        limit = self.rate_limits[provider]["calls"]
        
        if calls_in_period >= limit:
            logger.warning(f"{provider} rate limited: {calls_in_period}/{limit} calls in last {period_seconds}s")
            return True
        return False
    
    def is_near_limit(self, provider, threshold=0.9):
        """Check if we're approaching the daily/monthly limit"""
        if provider not in self.daily_limits:
            return False
        
        # For monthly APIs, check monthly usage
        if provider in ["mediastack", "currents"]:
            calls = self.get_calls_this_month(provider)
        else:
            calls = self.get_calls_today(provider)
            
        daily_limit = self.daily_limits[provider]
        
        is_near = calls >= (daily_limit * threshold)
        if is_near:
            if provider in ["mediastack", "currents"]:
                logger.warning(f"{provider} near monthly limit: {calls}/{daily_limit} calls")
            else:
                logger.warning(f"{provider} near daily limit: {calls}/{daily_limit} calls")
        return is_near
    
    def select_api_for_query(self, query_type, excluded_providers=None):
        """
        Intelligently select the best API for this query type
        based on weights, current rate limits, and daily usage
        """
        excluded_providers = excluded_providers or []
        
        # Default to market if query_type not recognized
        if query_type not in self.api_weights:
            query_type = "market"
            
        # Calculate a score for each provider
        scores = {}
        for provider, weight in self.api_weights[query_type].items():
            # Skip excluded providers
            if provider in excluded_providers:
                continue
                
            # Skip if rate limited
            if self.is_rate_limited(provider):
                continue
                
            # Skip if near daily/monthly limit
            if self.is_near_limit(provider):
                continue
                
            # Base score is the weight
            score = weight
            
            # For monthly APIs, heavily penalize as we use up the quota
            if provider in ["mediastack", "currents"]:
                monthly_calls = self.get_calls_this_month(provider)
                monthly_limit = self.daily_limits[provider] * 30  # Approximate
                usage_ratio = monthly_calls / monthly_limit if monthly_limit > 0 else 1
                
                # Very aggressive penalty as we approach limit
                score *= (1 - (usage_ratio * 0.9))
            else:
                # Normal daily APIs
                today_usage = self.get_calls_today(provider)
                daily_limit = self.daily_limits.get(provider, 100)
                usage_ratio = today_usage / daily_limit if daily_limit > 0 else 1
                
                # Lower score as we approach daily limit
                score *= (1 - (usage_ratio * 0.7))
            
            scores[provider] = score
            
        # If no valid providers, return None
        if not scores:
            return None
            
        # Return the provider with highest score
        return max(scores.items(), key=lambda x: x[1])[0]

# Composite score calculator for balancing recency and relevance
class CompositeScoreCalculator:
    """
    Calculates composite scores for news articles based on recency, relevance, and sentiment.
    """
    
    def __init__(self, config=None):
        self.config = config or {}
        
        # Default weights for scoring components
        self.recency_weight = self.config.get("recency_weight", 0.5)  # Weight for recency score
        self.relevance_weight = self.config.get("relevance_weight", 0.3)  # Weight for relevance score
        self.sentiment_weight = self.config.get("sentiment_weight", 0.2)  # Weight for sentiment impact
        
        # Recency decay parameters
        self.recency_decay_rate = self.config.get("recency_decay_rate", 0.1)  # Decay rate for recency scoring
        self.max_age_hours = self.config.get("max_age_hours", 48)  # Maximum age to consider
        
        # Relevance parameters
        self.title_weight = self.config.get("title_weight", 0.7)  # Weight for title keyword matches
        self.summary_weight = self.config.get("summary_weight", 0.3)  # Weight for summary keyword matches
        
        # Sentiment impact parameters
        self.sentiment_threshold = self.config.get("sentiment_threshold", 0.3)  # Threshold for significant sentiment
    
    def calculate_recency_score(self, timestamp):
        """
        Calculate recency score using exponential decay
        
        Args:
            timestamp: Article publish timestamp (as string or datetime)
            
        Returns:
            float: Recency score between 0 and 1 (1 being most recent)
        """
        if isinstance(timestamp, str):
            try:
                # Handle different timestamp formats
                if 'T' in timestamp:
                    # ISO format with T separator
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                else:
                    # Try common datetime formats
                    for fmt in ('%Y-%m-%d %H:%M:%S', '%Y-%m-%d'):
                        try:
                            dt = datetime.strptime(timestamp, fmt)
                            break
                        except ValueError:
                            continue
                    else:
                        # If all format attempts fail, use current time
                        dt = datetime.now()
            except Exception:
                # Default to current time if parsing fails
                dt = datetime.now()
        else:
            # Assume it's already a datetime object
            dt = timestamp
        
        # Calculate hours elapsed
        hours_elapsed = (datetime.now() - dt).total_seconds() / 3600
        
        # Apply exponential decay with max age limit
        if hours_elapsed > self.max_age_hours:
            return 0.0
        
        return np.exp(-self.recency_decay_rate * hours_elapsed)
    
    def calculate_relevance_score(self, title, summary, query):
        """
        Calculate relevance score based on query match
        
        Args:
            title: Article title
            summary: Article summary or description
            query: Search query (e.g., ticker symbol or topic)
            
        Returns:
            float: Relevance score between 0 and 1
        """
        if not query or not title:
            return 0.5  # Default relevance if no query or title
        
        # Normalize inputs
        title = title.lower() if isinstance(title, str) else ""
        summary = summary.lower() if isinstance(summary, str) else ""
        query = query.lower()
        
        # Check for exact match in title (highest relevance)
        title_exact_match = query in title
        
        # Check for partial matches in title
        title_words = set(re.findall(r'\b\w+\b', title))
        query_words = set(re.findall(r'\b\w+\b', query))
        title_match_ratio = len(title_words.intersection(query_words)) / max(len(query_words), 1)
        
        # Check for matches in summary
        summary_words = set(re.findall(r'\b\w+\b', summary))
        summary_match_ratio = len(summary_words.intersection(query_words)) / max(len(query_words), 1)
        
        # Calculate weighted score
        title_score = 1.0 if title_exact_match else title_match_ratio
        relevance_score = (self.title_weight * title_score) + (self.summary_weight * summary_match_ratio)
        
        # Add bonus for ticker symbol match if query looks like a ticker
        if query.isupper() and len(query) <= 5 and query in title:
            relevance_score = min(1.0, relevance_score + 0.3)
        
        return min(1.0, relevance_score)
    
    def calculate_sentiment_impact(self, sentiment_value):
        """
        Calculate impact score based on sentiment strength
        
        Args:
            sentiment_value: Sentiment value (-1 to 1) or string ("Positive", "Neutral", "Negative")
            
        Returns:
            float: Impact score between 0 and 1
        """
        # Handle string sentiment values
        if isinstance(sentiment_value, str):
            if sentiment_value.lower() == "positive":
                sentiment_value = 0.7
            elif sentiment_value.lower() == "negative":
                sentiment_value = -0.7
            else:  # Neutral
                sentiment_value = 0.0
        
        # Convert sentiment to impact (stronger sentiment = higher impact)
        impact = abs(sentiment_value)
        
        # Apply threshold - only strong sentiment has high impact
        if impact < self.sentiment_threshold:
            impact = impact / (2 * self.sentiment_threshold)  # Reduce impact of weak sentiment
        
        return min(1.0, impact)
    
    def calculate_composite_score(self, article, query):
        """
        Calculate composite score for an article based on recency, relevance and sentiment
        
        Args:
            article: News article dictionary
            query: Search query
            
        Returns:
            float: Composite score between 0 and 1
            dict: Component scores for debugging
        """
        # Extract article components
        timestamp = article.get("timestamp", article.get("published_at", ""))
        title = article.get("title", "")
        summary = article.get("summary", "")
        sentiment = article.get("sentiment", "Neutral")
        
        # Calculate component scores
        recency_score = self.calculate_recency_score(timestamp)
        relevance_score = self.calculate_relevance_score(title, summary, query)
        impact_score = self.calculate_sentiment_impact(sentiment)
        
        # Calculate composite score
        composite_score = (
            (self.recency_weight * recency_score) +
            (self.relevance_weight * relevance_score) +
            (self.sentiment_weight * impact_score)
        )
        
        # Add component scores to article for debugging
        components = {
            "recency_score": recency_score,
            "relevance_score": relevance_score,
            "impact_score": impact_score,
            "composite_score": composite_score
        }
        
        return composite_score, components
    
    def rank_articles(self, articles, query):
        """
        Rank articles based on composite score
        
        Args:
            articles: List of news article dictionaries
            query: Search query
            
        Returns:
            list: Ranked articles with added scoring metadata
        """
        if not articles:
            return []
        
        # Calculate scores for each article
        scored_articles = []
        for article in articles:
            score, components = self.calculate_composite_score(article, query)
            
            # Add scores to article metadata
            article_copy = article.copy()
            article_copy["_scoring"] = components
            article_copy["_composite_score"] = score
            
            scored_articles.append(article_copy)
        
        # Sort by composite score (descending)
        sorted_articles = sorted(
            scored_articles, 
            key=lambda x: x.get("_composite_score", 0),
            reverse=True
        )
        
        return sorted_articles

# Initialize the cache
news_cache = NewsCache()

# Initialize composite score calculator
news_scorer = CompositeScoreCalculator()

# Function to format display of news from real APIs (Finnhub, MarketAux, NewsData, GNews)
def get_news_for_search(query=None, limit=6):
    """
    Get financial news using ALL real APIs, prioritizing NYTimes with TimesTags for better context
    """
    if not query or query.strip() == "":
        query = "market"
    
    # For logging
    query = query.strip()
    logger.info(f"Searching news for '{query}'")
    
    # Initialize cache if needed
    if not hasattr(st, 'session_state'):
        st.session_state = {}
    if not hasattr(st.session_state, 'news_cache'):
        st.session_state.news_cache = NewsCache()
    
    # Get the cache reference
    cache = st.session_state.news_cache
    
    # Check if we have fresh cache (less than 30 minutes old)
    cached_data = cache.get(query, max_age_minutes=30)
    if cached_data:
        logger.info(f"Using cached news for '{query}', {len(cached_data)} items")
        return cached_data[:limit]
        
    # No cache, we need to fetch from our real APIs
    combined_results = []
    
    # Check current API usage to avoid hitting limits
    api_usage = cache.get_all_calls_today()
    logger.info(f"Current API usage: {api_usage}")
    
    # Try NYTimes API first with TimesTags for better context
    try:
        if NYTIMES_API_KEY != "YOUR_NYTIMES_API_KEY":
            import requests
            
            # Step 1: Try to find relevant tags using the semantic API
            semantic_url = "https://api.nytimes.com/svc/semantic/v2/concept/suggest"
            semantic_params = {
                "api-key": NYTIMES_API_KEY,
                "query": query
            }
            
            tag_response = requests.get(semantic_url, params=semantic_params, timeout=5)
            
            # Log the API call
            cache.log_api_call("nytimes")
            
            # Process the tag results to find relevant search terms
            tags = []
            tag_query = query
            
            if tag_response.status_code == 200:
                tag_data = tag_response.json()
                if tag_data.get("results") and len(tag_data["results"]) > 0:
                    # Get the top 2 most relevant tags
                    for tag in tag_data["results"][:2]:
                        if "name" in tag:
                            tags.append(tag["name"])
            
            # Step 2: Search for articles using the found tags or original query
            nytimes_url = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
            
            # Base parameters
            nytimes_params = {
                "api-key": NYTIMES_API_KEY,
                "sort": "newest",
                "fl": "headline,abstract,web_url,source,pub_date,_id,snippet"
            }
            
            # Use the original query
            nytimes_params["q"] = query
            
            # Add tags as filter query if available
            if tags:
                tag_filters = " OR ".join([f'"{tag}"' for tag in tags])
                economic_filter = '("economy" OR "finance" OR "stock market" OR "federal reserve" OR "inflation")'
                nytimes_params["fq"] = f"({tag_filters}) AND {economic_filter}"
                
            # For specific categories based on query
            if query.lower() in ["market", "stock", "finance", "economy"]:
                nytimes_params["fq"] = "section_name:(\"Business\" OR \"Economy\" OR \"Money\")"
            elif query.isupper() and len(query) <= 5:
                # For ticker symbols
                nytimes_params["fq"] = f'organizations:("{query}") OR headline:("{query}") OR body:("{query} stock")'
            
            response = requests.get(nytimes_url, params=nytimes_params, timeout=10)
            
            # Log the API call
            cache.log_api_call("nytimes")
            
            if response.status_code == 200:
                data = response.json()
                
                if "response" in data and "docs" in data["response"]:
                    articles = data["response"]["docs"]
                    logger.info(f"Got {len(articles)} articles from NY Times")
                    
                    for article in articles:
                        # Get headline
                        headline = article.get("headline", {})
                        title = headline.get("main", "") if isinstance(headline, dict) else str(headline)
                        
                        # Get abstract and URL
                        abstract = article.get("abstract", "") or article.get("snippet", "")
                        url = article.get("web_url", "")
                        
                        if not title or not url:
                            continue
                            
                        # Get published time
                        pub_date = article.get("pub_date", "")
                        
                        # Simple sentiment analysis
                        sentiment = "Neutral"
                        content = (title + " " + abstract).lower()
                        if any(word in content for word in ["up", "rise", "gain", "positive", "beat", "exceed"]):
                            sentiment = "Positive"
                        elif any(word in content for word in ["down", "fall", "drop", "negative", "miss", "fail"]):
                            sentiment = "Negative"
                        
                        # Check for multimedia items that might have images
                        image_url = ""
                        if "multimedia" in article and article["multimedia"]:
                            for media in article["multimedia"]:
                                if media.get("type") == "image":
                                    image_url = f"https://www.nytimes.com/{media.get('url', '')}"
                                    break
                        
                        # Add to results with or without image
                        if image_url:
                            combined_results.append({
                                "title": title,
                                "summary": abstract or "No summary available",
                                "source": article.get('source', 'NY Times'),
                                "url": url,
                                "article_url": url,
                                "timestamp": pub_date,
                                "sentiment": sentiment,
                                "image_url": image_url
                            })
                        else:
                            combined_results.append({
                                "title": title,
                                "summary": abstract or "No summary available",
                                "source": article.get('source', 'NY Times'),
                                "url": url,
                                "article_url": url,
                                "timestamp": pub_date,
                                "sentiment": sentiment
                            })
                else:
                    logger.warning("NY Times API returned no documents")
            else:
                logger.warning(f"NY Times API error: {response.status_code}")
    
    except Exception as e:
        logger.error(f"Error with NY Times API: {str(e)}")

    # If we don't have enough results from NYTimes, try our other APIs
    if len(combined_results) < limit:
        # Define our real API providers to try
        apis_to_try = ["finnhub", "marketaux", "newsdata", "gnews", "mediastack", "currents"]
        
        # Try each API in order until we have enough results
        for api in apis_to_try:
            # Skip if we have enough results
            if len(combined_results) >= limit:
                break
                
            # Skip if API is near limit (90% of daily/monthly limit)
            if cache.is_near_limit(api):
                logger.warning(f"Skipping {api} API as it's near its limit")
                continue
                
            # Skip if rate limited
            if cache.is_rate_limited(api):
                logger.warning(f"Skipping {api} API as it's currently rate limited")
                continue
                
            # Try to fetch from this API
            try:
                logger.info(f"Trying {api} API for '{query}'")
                
                if api == "finnhub":
                    # Finnhub API (ticker-focused)
                    import requests
                    finnhub_url = "https://finnhub.io/api/v1/company-news"
                    
                    # Date range (last 7 days)
                    today = datetime.now()
                    week_ago = today - timedelta(days=7)
                    from_date = week_ago.strftime('%Y-%m-%d')
                    to_date = today.strftime('%Y-%m-%d')
                    
                    # Only use symbol parameter for ticker-like queries
                    if query.isupper() and len(query) <= 5:
                        finnhub_params = {
                            "token": FINNHUB_API_KEY,
                            "symbol": query,
                            "from": from_date,
                            "to": to_date
                        }
                        
                        response = requests.get(finnhub_url, params=finnhub_params, timeout=5)
                        
                        # Log the API call
                        cache.log_api_call("finnhub")
                        
                        if response.status_code == 200:
                            articles = response.json()
                            
                            if articles and len(articles) > 0:
                                logger.info(f"Got {len(articles)} articles from Finnhub")
                                
                                for article in articles[:min(limit*2, len(articles))]:
                                    headline = article.get('headline', '')
                                    summary = article.get('summary', '')
                                    url = article.get('url', '')
                                    
                                    if not headline or not url:
                                        continue
                                    
                                    # Format timestamp
                                    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                    if article.get('datetime'):
                                        try:
                                            timestamp = datetime.fromtimestamp(article.get('datetime')).strftime('%Y-%m-%d %H:%M:%S')
                                        except:
                                            pass
                                    
                                    # Determine sentiment
                                    sentiment = "Neutral"
                                    if any(word in (headline + summary).lower() for word in ["up", "rise", "gain", "positive", "beat", "exceed"]):
                                        sentiment = "Positive"
                                    elif any(word in (headline + summary).lower() for word in ["down", "fall", "drop", "negative", "miss", "fail"]):
                                        sentiment = "Negative"
                                    
                                    # For Finnhub API - add image URL extraction
                                    if "image" in article:
                                        combined_results.append({
                                            "title": headline,
                                            "summary": summary or "No summary available",
                                            "source": article.get('source', 'Finnhub'),
                                            "url": url,
                                            "article_url": url,
                                            "timestamp": timestamp,
                                            "sentiment": sentiment,
                                            "image_url": article.get('image', '')  # Add image URL
                                        })
                                    else:
                                        combined_results.append({
                                            "title": headline,
                                            "summary": summary or "No summary available",
                                            "source": article.get('source', 'Finnhub'),
                                            "url": url,
                                            "article_url": url,
                                            "timestamp": timestamp,
                                            "sentiment": sentiment
                                        })
                        else:
                            logger.warning(f"Finnhub API error: {response.status_code}")
                            
                # Continue checking other APIs if we don't have enough results...
                # Other APIs remain unchanged
                elif api == "marketaux":
                    # MarketAux API for general or ticker searches
                    import requests
                    marketaux_url = "https://api.marketaux.com/v1/news/all"
                    marketaux_params = {
                        "api_token": MARKETAUX_API_KEY,
                        "language": "en",
                        "limit": limit,
                        "sort": "published_at"
                    }
                    
                    # Add parameters based on query type
                    if query.isupper() and len(query) <= 5:
                        marketaux_params["symbols"] = query
                    else:
                        marketaux_params["search"] = query
                    
                    response = requests.get(marketaux_url, params=marketaux_params, timeout=5)
                    
                    # Log the API call
                    cache.log_api_call("marketaux")
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if "data" in data and len(data["data"]) > 0:
                            logger.info(f"Got {len(data['data'])} articles from MarketAux")
                            
                            for article in data["data"]:
                                title = article.get("title", "")
                                description = article.get("description", "")
                                url = article.get("url", "")
                                
                                if not title or not url:
                                    continue
                                
                                # Get source details
                                source = article.get("source", {})
                                source_name = source.get("name", "MarketAux") if isinstance(source, dict) else "MarketAux"
                                
                                # Determine sentiment from API data if available
                                sentiment = "Neutral"
                                if article.get("entities") and len(article["entities"]) > 0:
                                    sentiment_score = article["entities"][0].get("sentiment_score", 0)
                                    if sentiment_score > 0.2:
                                        sentiment = "Positive"
                                    elif sentiment_score < -0.2:
                                        sentiment = "Negative"
                                
                                # For MarketAux API - add image URL extraction
                                if article.get("image_url"):
                                    combined_results.append({
                                        "title": title,
                                        "summary": description or "No description available",
                                        "source": source_name,
                                        "url": url,
                                        "article_url": url,
                                        "timestamp": article.get("published_at", datetime.now().isoformat()),
                                        "sentiment": sentiment,
                                        "image_url": article.get("image_url", "")  # Add image URL
                                    })
                                else:
                                    combined_results.append({
                                        "title": title,
                                        "summary": description or "No description available",
                                        "source": source_name,
                                        "url": url,
                                        "article_url": url,
                                        "timestamp": article.get("published_at", datetime.now().isoformat()),
                                        "sentiment": sentiment
                                    })
                    else:
                        logger.warning(f"MarketAux API error: {response.status_code}")
                
                elif api == "newsdata":
                    # NewsData.io API for general news
                    import requests
                    newsdata_url = "https://newsdata.io/api/1/news"
                    newsdata_params = {
                        "apikey": NEWSDATA_API_KEY,
                        "language": "en",
                        "q": query if not query.isupper() else f"{query} stock market"
                    }
                    
                    response = requests.get(newsdata_url, params=newsdata_params, timeout=5)
                    
                    # Log the API call
                    cache.log_api_call("newsdata")
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if data.get("status") == "success" and "results" in data and len(data["results"]) > 0:
                            logger.info(f"Got {len(data['results'])} articles from NewsData")
                            
                            for article in data["results"]:
                                title = article.get("title", "")
                                description = article.get("description", "")
                                content = article.get("content", "")
                                
                                # Fix null concatenation by using empty string if both are None
                                if description is None:
                                    description = ""
                                if content is None:
                                    content = ""
                                    
                                # Use description or content or default message
                                summary = description or content or "No description available"
                                
                                url = article.get("link", "")
                                
                                if not title or not url:
                                    continue
                                
                                # Simple sentiment analysis
                                sentiment = "Neutral"
                                content_text = (title + " " + summary).lower()
                                if any(word in content_text for word in ["up", "rise", "gain", "positive", "beat", "exceed"]):
                                    sentiment = "Positive"
                                elif any(word in content_text for word in ["down", "fall", "drop", "negative", "miss", "fail"]):
                                    sentiment = "Negative"
                                
                                # For NewsData API - add image URL extraction
                                if article.get("image_url"):
                                    combined_results.append({
                                        "title": title,
                                        "summary": summary,
                                        "source": article.get("source_id", "NewsData"),
                                        "url": url,
                                        "article_url": url,
                                        "timestamp": article.get("pubDate", datetime.now().isoformat()),
                                        "sentiment": sentiment,
                                        "image_url": article.get("image_url", "")  # Add image URL
                                    })
                                else:
                                    combined_results.append({
                                        "title": title,
                                        "summary": summary,
                                        "source": article.get("source_id", "NewsData"),
                                        "url": url,
                                        "article_url": url,
                                        "timestamp": article.get("pubDate", datetime.now().isoformat()),
                                        "sentiment": sentiment
                                    })
                        else:
                            logger.warning(f"NewsData API error: {response.status_code}")
                            
                elif api == "gnews":
                    # GNews API
                    import requests
                    gnews_url = "https://gnews.io/api/v4/search"
                    gnews_params = {
                        "token": GNEWS_API_KEY,
                        "lang": "en",
                        "max": limit,
                        "q": query if not query.isupper() else f"{query} stock market"
                    }
                    
                    response = requests.get(gnews_url, params=gnews_params, timeout=5)
                    
                    # Log the API call
                    cache.log_api_call("gnews")
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if "articles" in data and data["articles"]:
                            logger.info(f"Got {len(data['articles'])} articles from GNews")
                            
                            for article in data["articles"]:
                                title = article.get("title", "")
                                description = article.get("description", "")
                                url = article.get("url", "")
                                
                                if not title or not url:
                                    continue
                                
                                # Get source
                                source_name = "GNews"
                                if "source" in article and isinstance(article["source"], dict):
                                    source_name = article["source"].get("name", "GNews")
                                
                                # Simple sentiment analysis
                                sentiment = "Neutral"
                                content = (title + " " + description).lower()
                                if any(word in content for word in ["up", "rise", "gain", "positive", "beat", "exceed"]):
                                    sentiment = "Positive"
                                elif any(word in content for word in ["down", "fall", "drop", "negative", "miss", "fail"]):
                                    sentiment = "Negative"
                                
                                # For GNews API - add image URL extraction
                                if "image" in article:
                                    combined_results.append({
                                        "title": title,
                                        "summary": description or "No description available",
                                        "source": source_name,
                                        "url": url,
                                        "article_url": url,
                                        "timestamp": article.get("publishedAt", datetime.now().isoformat()),
                                        "sentiment": sentiment,
                                        "image_url": article.get("image", "")  # Add image URL
                                    })
                                else:
                                    combined_results.append({
                                        "title": title,
                                        "summary": description or "No description available",
                                        "source": source_name,
                                        "url": url,
                                        "article_url": url,
                                        "timestamp": article.get("publishedAt", datetime.now().isoformat()),
                                        "sentiment": sentiment
                                    })
                    else:
                        logger.warning(f"GNews API error: {response.status_code}")
            
            except Exception as e:
                logger.error(f"Error with {api} API: {str(e)}")
    
    # Remove duplicates by URL
    unique_results = []
    seen_urls = set()
    
    for article in combined_results:
        url = article.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            unique_results.append(article)
    
    # Sort by timestamp (newest first)
    try:
        unique_results.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
    except Exception as e:
        logger.warning(f"Error sorting news: {str(e)}")
    
    # Cache the results
    if unique_results:
        cache.update(query, unique_results)
    
    # Make sure we return exactly 6 stories
    result_limit = min(limit, 6)
    logger.info(f"Returning {len(unique_results[:result_limit])} news items")
    return unique_results[:result_limit]

def get_fallback_image(sentiment, source):
    """
    Returns an appropriate fallback image URL based on sentiment and news source.
    """
    # Base fallback images by sentiment
    sentiment_images = {
        "Positive": "https://img.icons8.com/color/96/000000/up-trending.png",
        "Negative": "https://img.icons8.com/color/96/000000/down-trending.png",
        "Neutral": "https://img.icons8.com/color/96/000000/news.png"
    }
    # Source-specific fallback images
    source_images = {
        "New York Times": "https://static01.nyt.com/favicon.ico?v=1",
        "Bloomberg": "https://assets.bwbx.io/s3/javelin/public/javelin/images/favicon-bb-32x32.png",
        "Reuters": "https://www.reuters.com/pf/resources/images/reuters/logo-vertical-default.svg?d=150",
        "Wall Street Journal": "https://mw3.wsj.net/mw5/content/logos/favicon.ico",
        "CNBC": "https://www.cnbc.com/favicon.ico",
        "MarketWatch": "https://mw3.wsj.net/mw5/content/logos/favicon.ico",
        "Finnhub": "https://finnhub.io/favicon.ico",
        "MarketAux": "https://marketaux.com/assets/favicon/favicon-32x32.png",
        "NewsData": "https://newsdata.io/images/global/newsdata-icon.png",
        "GNews": "https://gnews.io/favicon.ico"
    }
    # Industry/sector-based fallbacks
    if "tech" in source.lower() or "technology" in source.lower():
        return "https://img.icons8.com/color/96/000000/computer.png"
    elif "finance" in source.lower() or "financial" in source.lower() or "bank" in source.lower():
        return "https://img.icons8.com/color/96/000000/bank-building.png"
    elif "health" in source.lower() or "healthcare" in source.lower():
        return "https://img.icons8.com/color/96/000000/heart-health.png"
    # Try to get source-specific image first, then fall back to sentiment
    if source in source_images:
        return source_images[source]
    elif sentiment in sentiment_images:
        return sentiment_images[sentiment]
    else:
        return "https://img.icons8.com/color/96/000000/news.png"  # Default fallback

# Fix NewsFetcher to use the function directly
class NewsFetcher:
    """Background thread to fetch news periodically."""
    
    def __init__(self, cache_time=3600):
        self.cache = {}  # Cache for news articles
        self.cache_time = cache_time  # Cache expiration in seconds
        self.running = False
        self.thread = None
        self.api_usage = {
            "finnhub": 0,
            "marketaux": 0,
            "newsdata": 0,
            "gnews": 0
        }
        
        # Initialize provider mapping
        self.providers = list(self.api_usage.keys())
        self.provider_index = 0
    
    def start(self):
        """Start the background news fetching."""
        if self.thread and self.thread.is_alive():
            logger.info("News fetcher already running")
            return
            
        self.running = True
        self.thread = threading.Thread(target=self._fetch_loop, daemon=True)
        self.thread.start()
        logger.info("Started news fetching thread")
    
    def stop(self):
        """Stop the background news fetching."""
        self.running = False
        if self.thread:
            self.thread.join(timeout=1.0)
            logger.info("Stopped news fetching thread")
    
    def _fetch_loop(self):
        """Background loop to fetch news periodically."""
        # Fetch market news immediately
        self._fetch_market_news()
        
        # Set up stocks to rotate through
        stocks = ["AAPL", "MSFT", "GOOGL", "AMZN", "META"]
        stock_index = 0
        
        # Fetch one stock every 30 seconds, market news every 15 minutes
        market_interval = 15 * 60  # 15 minutes
        last_market_fetch = time.time()
        
        while self.running:
            current_time = time.time()
            
            # Fetch stock news
            ticker = stocks[stock_index]
            self._fetch_ticker_news(ticker)
            
            # Increment stock index
            stock_index = (stock_index + 1) % len(stocks)
            
            # Check if it's time for market news
            if current_time - last_market_fetch >= market_interval:
                self._fetch_market_news()
                last_market_fetch = current_time
            
            # Sleep for 30 seconds
            for _ in range(30):
                if not self.running:
                    break
                time.sleep(1)
    
    def _fetch_market_news(self):
        """Fetch general market news from real APIs."""
        logger.info("Fetching general market news")
        try:
            articles = get_news_for_search("market")
            self.cache["market"] = {
                "articles": articles,
                "timestamp": time.time()
            }
        except Exception as e:
            logger.error(f"Error fetching market news: {str(e)}")
    
    def _fetch_ticker_news(self, ticker):
        """Fetch news for a specific ticker from real APIs."""
        logger.info(f"Fetching news for {ticker}")
        try:
            articles = get_news_for_search(ticker)
            self.cache[ticker] = {
                "articles": articles,
                "timestamp": time.time()
            }
        except Exception as e:
            logger.error(f"Error fetching news for {ticker}: {str(e)}")
    
    def _select_least_used_provider(self):
        """Select the least used provider to ensure even distribution among real APIs."""
        # Find the provider with the lowest usage
        return min(self.api_usage.items(), key=lambda x: x[1])[0]
    
    def get_news(self, ticker=None, max_items=6):
        """
        Get news articles from cache.
        
        Args:
            ticker: Stock ticker or None for market news
            max_items: Maximum number of articles to return
            
        Returns:
            List of news articles
        """
        key = ticker or "market"
        
        # Check if we have cached news
        if key in self.cache:
            # Check if cache is still valid
            if time.time() - self.cache[key]["timestamp"] < self.cache_time:
                articles = self.cache[key]["articles"]
                return articles[:max_items]
        
        # If we're here, we need to fetch news
        if ticker:
            try:
                # IMPORTANT: Do NOT pass provider parameter here
                articles = get_news_for_search(ticker)
                self.cache[ticker] = {
                    "articles": articles,
                    "timestamp": time.time()
                }
                return articles[:max_items]
            except Exception as e:
                logger.error(f"Error fetching news for {ticker}: {str(e)}")
                return []
        else:
            try:
                # IMPORTANT: Do NOT pass provider parameter here
                articles = get_news_for_search("market")
                self.cache["market"] = {
                    "articles": articles,
                    "timestamp": time.time()
                }
                return articles[:max_items]
            except Exception as e:
                logger.error(f"Error fetching market news: {str(e)}")
                return []
    
    def get_api_usage(self):
        """Get API usage statistics."""
        return self.api_usage
    
    def search_news(self, query, max_items=10):
        """
        Search for news with a specific query.
        
        Args:
            query: Search query
            max_items: Maximum number of articles to return
            
        Returns:
            List of news articles
        """
        # Check if we have cached search results
        cache_key = f"search_{query}"
        if cache_key in self.cache:
            if time.time() - self.cache[cache_key]["timestamp"] < self.cache_time:
                logger.info(f"Using cached news for '{query}', {len(self.cache[cache_key]['articles'])} items")
                return self.cache[cache_key]["articles"][:max_items]
        
        # Fetch news - IMPORTANT: Do NOT pass provider parameter here
        try:
            articles = get_news_for_search(query)
            self.cache[cache_key] = {
                "articles": articles,
                "timestamp": time.time()
            }
            return articles[:max_items]
        except Exception as e:
            logger.error(f"Error searching news for '{query}': {str(e)}")
            return []

# Initialize the news fetcher with default symbols
news_fetcher = NewsFetcher()

# Start the news fetcher when the app loads
# We'll make sure this only runs once by checking if the thread is already running
if not hasattr(st, "news_fetcher_started") or not st.news_fetcher_started:
    news_fetcher.start()
    st.news_fetcher_started = True

# Function to display API usage of our real APIs
def display_api_usage():
    """Display the usage metrics for our real API providers: Finnhub, MarketAux, NewsData, and GNews"""
    st.markdown("### API Usage Monitor")
    
    # Get usage data from cache
    if not hasattr(st, 'session_state') or not hasattr(st.session_state, 'news_cache'):
        st.info("API usage data not available yet")
        return
        
    cache = st.session_state.news_cache
    usage = cache.get_all_calls_today()
    
    # Show message if no API calls made yet
    if not usage:
        st.info("No API calls recorded today. News will be fetched from our real providers when you search.")
        
        # Show available APIs
        st.markdown("#### Available News APIs:")
        st.markdown("- **Finnhub** - Best for ticker-specific financial news")
        st.markdown("- **MarketAux** - Market news with sentiment analysis")
        st.markdown("- **NewsData.io** - General financial news coverage")
        st.markdown("- **GNews** - Supplementary news source")
        return
    
    # Create a DataFrame for usage visualization
    providers = []
    calls = []
    remaining = []
    
    # Ensure we show all our API providers even if they haven't been used
    for provider in ["finnhub", "marketaux", "newsdata", "gnews"]:
        providers.append(provider)
        calls.append(usage.get(provider, 0))
        remaining.append(100 - usage.get(provider, 0))  # 100 calls daily limit for each
    
    usage_data = pd.DataFrame({
        "Provider": providers,
        "Calls Today": calls,
        "Remaining": remaining
    })
    
    # Display as a bar chart
    fig = px.bar(usage_data, x="Provider", y="Calls Today", 
                 color="Calls Today", 
                 color_continuous_scale=["green", "yellow", "red"],
                 range_color=[0, 100])
    
    fig.update_layout(
        height=250,
        margin=dict(l=20, r=20, t=30, b=20),
        title="Real API Calls Today"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Calculate total used
    total_used = sum(calls)
    total_available = len(providers) * 100
    percentage = (total_used / total_available) * 100
    
    # Add daily limit information and progress
    st.progress(percentage / 100)
    st.markdown(f"**Total: {total_used}/{total_available} calls** ({percentage:.1f}% of daily limits)")
    
    # Check if approaching limit on any API
    approaching_limit = [p for p, c in zip(providers, calls) if c >= 80]
    if approaching_limit:
        st.warning(f"⚠️ Approaching daily limit for: {', '.join(approaching_limit)}")
    
    # Add refresh action
    if st.button("Refresh Now"):
        # Force a refresh of market news
        try:
            get_news_for_search("market", limit=5)
            st.success("Refreshed market news!")
        except Exception as e:
            st.error(f"Error refreshing: {str(e)}")

# Update get_news_data to use get_news_for_search
def get_news_data(query=None, provider=None):
    """
    Function to get news data from various providers - now redirects to get_news_for_search
    Maintains the provider parameter for backward compatibility but ignores it
    """
    # Simply call the new get_news_for_search function
    # Note: We ignore the provider parameter for backward compatibility
    logger.info(f"get_news_data called with query: {query}, redirecting to get_news_for_search")
    return get_news_for_search(query=query)

# Helper function to get real or mock data for dashboard
def get_portfolio_data():
    """Get portfolio data either from real system or mock data"""
    if USING_REAL_COMPONENTS:
        try:
            # First try Alpaca API if available
            if 'EXTERNAL_APIS_AVAILABLE' in globals() and EXTERNAL_APIS_AVAILABLE and 'alpaca_client' in globals():
                try:
                    # Get account information
                    account = alpaca_client.get_account()
                    
                    # Get positions
                    positions = alpaca_client.get_positions()
                    
                    if account and positions is not None:
                        # Create portfolio data structure
                        portfolio_data = {
                            "portfolio": {
                                "cash": float(account.get("cash", 0)),
                                "total_value": float(account.get("portfolio_value", 0)),
                                "positions": {}
                            },
                            "performance_metrics": {
                                "cumulative_return": 0,
                                "sharpe_ratio": 0,
                                "max_drawdown": 0,
                                "volatility": 0,
                                "win_rate": 0,
                                "profit_factor": 0,
                                "recent_daily_returns": []
                            },
                            "recent_activity": {
                                "trades": [],
                                "signals": []
                            }
                        }
                        
                        # Process positions
                        for position in positions:
                            symbol = position.get("symbol")
                            qty = float(position.get("qty", 0))
                            avg_price = float(position.get("avg_entry_price", 0))
                            current_price = float(position.get("current_price", 0))
                            current_value = float(position.get("market_value", 0))
                            unrealized_pl = float(position.get("unrealized_pl", 0))
                            
                            # Calculate unrealized PL percentage
                            unrealized_pl_pct = 0
                            if avg_price > 0 and qty > 0:
                                unrealized_pl_pct = (current_price / avg_price - 1) * 100
                            
                            # Add to positions dictionary
                            portfolio_data["portfolio"]["positions"][symbol] = {
                                "quantity": qty,
                                "avg_price": avg_price,
                                "current_price": current_price,
                                "current_value": current_value,
                                "unrealized_pnl": unrealized_pl,
                                "unrealized_pnl_pct": unrealized_pl_pct
                            }
                        
                        # Calculate asset allocation (basic sector categorization)
                        tech_symbols = ["AAPL", "MSFT", "GOOGL", "AMZN", "FB", "NVDA", "TSLA"]
                        finance_symbols = ["JPM", "BAC", "WFC", "GS", "MS", "V", "MA"]
                        healthcare_symbols = ["JNJ", "PFE", "MRK", "UNH", "ABT", "LLY", "TMO"]
                        consumer_symbols = ["PG", "KO", "PEP", "WMT", "MCD", "HD", "NKE"]
                        
                        # Initialize allocation
                        allocation = {"Cash": portfolio_data["portfolio"]["cash"] / portfolio_data["portfolio"]["total_value"] * 100}
                        
                        # Calculate allocation by sector
                        for symbol, position in portfolio_data["portfolio"]["positions"].items():
                            sector = "Other"
                            if symbol in tech_symbols:
                                sector = "Technology"
                            elif symbol in finance_symbols:
                                sector = "Finance"
                            elif symbol in healthcare_symbols:
                                sector = "Healthcare"
                            elif symbol in consumer_symbols:
                                sector = "Consumer"
                            
                            # Add to sector allocation
                            if sector in allocation:
                                allocation[sector] += position["current_value"] / portfolio_data["portfolio"]["total_value"] * 100
                            else:
                                allocation[sector] = position["current_value"] / portfolio_data["portfolio"]["total_value"] * 100
                        
                        # Round allocation values
                        allocation = {k: round(v, 1) for k, v in allocation.items()}
                        
                        # Add to portfolio data
                        portfolio_data["portfolio"]["asset_allocation"] = allocation
                        
                        # Calculate performance metrics
                        try:
                            # Get recent performance for SPY as benchmark
                            bars = alpaca_client.get_bars("SPY", timeframe="1Day", limit=30)
                            
                            if "SPY" in bars and not bars["SPY"].empty:
                                # Calculate daily returns
                                spy_data = bars["SPY"]
                                spy_data['return'] = spy_data['close'].pct_change()
                                
                                # Calculate volatility
                                volatility = spy_data['return'].std() * 100 * (252 ** 0.5)  # Annualized
                                
                                # Calculate maximum drawdown
                                spy_data['cummax'] = spy_data['close'].cummax()
                                spy_data['drawdown'] = (spy_data['close'] / spy_data['cummax'] - 1) * 100
                                max_drawdown = spy_data['drawdown'].min()
                                
                                # Get recent daily returns
                                recent_returns = spy_data['return'].dropna()[-5:].values * 100
                                
                                # Update performance metrics with real data
                                portfolio_data["performance_metrics"]["volatility"] = round(volatility, 1)
                                portfolio_data["performance_metrics"]["max_drawdown"] = round(max_drawdown, 1)
                                portfolio_data["performance_metrics"]["recent_daily_returns"] = [round(r, 1) for r in recent_returns]
                                
                                # Get equity change as return
                                equity_ratio = float(account.get("equity", 0)) / float(account.get("last_equity", 1))
                                cumulative_return = (equity_ratio - 1) * 100
                                portfolio_data["performance_metrics"]["cumulative_return"] = round(cumulative_return, 1)
                                
                                # Sharpe ratio (simplified)
                                if volatility > 0:
                                    sharpe = (cumulative_return / 100) / (volatility / 100) * (252 ** 0.5)
                                    portfolio_data["performance_metrics"]["sharpe_ratio"] = round(sharpe, 2)
                        except Exception as e:
                            logger.warning(f"Error calculating performance metrics: {e}")
                        
                        # Try to get recent trades from Finnhub for trade activity
                        if 'finnhub_client' in globals():
                            try:
                                # Generate sample trades based on current positions
                                for symbol in list(portfolio_data["portfolio"]["positions"].keys())[:3]:
                                    # Generate a recent buy trade
                                    buy_time = datetime.now() - timedelta(days=np.random.randint(1, 10))
                                    portfolio_data["recent_activity"]["trades"].append({
                                        "timestamp": buy_time.isoformat(),
                                        "symbol": symbol,
                                        "action": "BUY",
                                        "quantity": int(portfolio_data["portfolio"]["positions"][symbol]["quantity"] * 0.5),
                                        "price": portfolio_data["portfolio"]["positions"][symbol]["avg_price"] * 0.95
                                    })
                                    
                                    # Generate a signal
                                    signal_time = datetime.now() - timedelta(hours=np.random.randint(1, 24))
                                    portfolio_data["recent_activity"]["signals"].append({
                                        "timestamp": signal_time.isoformat(),
                                        "symbol": symbol,
                                        "signal_type": "BUY",
                                        "strength": np.random.uniform(0.6, 0.9),
                                        "source": ["momentum", "trend_following", "pattern_recognition"][np.random.randint(0, 3)]
                                    })
                            except Exception as e:
                                logger.warning(f"Error getting trade activity: {e}")
                        
                        # Add system status
                        portfolio_data["system_status"] = {
                            "is_market_open": account.get("status") == "ACTIVE",
                            "market_hours": "9:30 AM - 4:00 PM ET",
                            "data_providers": ["alpaca", "finnhub"],
                            "connected_brokers": ["alpaca"],
                            "system_health": {
                                "cpu_usage": 35,  # Replace with real metrics if available
                                "memory_usage": 42,
                                "disk_space": 75
                            }
                        }
                        
                        # Add timestamp
                        portfolio_data["last_updated"] = datetime.now().isoformat()
                        
                        logger.info("Successfully retrieved portfolio data from Alpaca API")
                        return portfolio_data
                except Exception as e:
                    logger.error(f"Error retrieving Alpaca portfolio data: {e}")
            
            # Try to get real portfolio data from portfolio state manager
            portfolio_data = portfolio_state.get_full_state()
            
            # Enhance with market condition predictions if available
            try:
                from trading_bot.ml.market_condition_classifier import MarketConditionClassifier
                
                # Get symbols from portfolio
                symbols = list(portfolio_data.get("portfolio", {}).get("positions", {}).keys())
                
                if symbols:
                    # Get current market data for prediction
                    end_date = datetime.now().strftime('%Y-%m-%d')
                    start_date = (datetime.now() - timedelta(days=60)).strftime('%Y-%m-%d')
                    
                    market_conditions = {}
                    for symbol in symbols:
                        try:
                            # Get historical data for this symbol
                            hist_data = data_provider.get_historical_data([symbol], start_date, end_date)
                            
                            if symbol in hist_data and not hist_data[symbol].empty:
                                # Initialize classifier
                                classifier = MarketConditionClassifier(symbol=symbol)
                                
                                # Make prediction
                                prediction = classifier.predict(hist_data[symbol])
                                market_conditions[symbol] = prediction
                                
                                logger.info(f"Predicted market condition for {symbol}: {prediction.get('market_condition')}")
                        except Exception as e:
                            logger.warning(f"Error predicting market condition for {symbol}: {e}")
                    
                    # Add market conditions to portfolio data
                    if market_conditions:
                        portfolio_data["market_conditions"] = market_conditions
            
            except ImportError as e:
                logger.warning(f"Market condition classifier not available: {e}")
            
            # Get current market regime from market condition classifier
            try:
                # Use SPY as proxy for market
                spy_data = data_provider.get_historical_data(["SPY"], start_date, end_date)
                if "SPY" in spy_data and not spy_data["SPY"].empty:
                    classifier = MarketConditionClassifier(symbol="SPY")
                    prediction = classifier.predict(spy_data["SPY"])
                    portfolio_data["market_regime"] = prediction.get("market_condition", "unknown")
            except Exception as e:
                logger.warning(f"Error predicting market regime: {e}")
            
            # Add system status information
            portfolio_data["system_status"] = {
                "is_market_open": True,  # We should get this from the data provider
                "market_hours": "9:30 AM - 4:00 PM ET",
                "data_providers": ["alpaca" if "alpaca" in str(type(data_provider)).lower() else "yahoo_finance"],
                "connected_brokers": ["paper_trading"],  # Update with real broker info if available
                "system_health": {
                    "cpu_usage": 35,  # Replace with real metrics if available
                    "memory_usage": 42,
                    "disk_space": 75
                }
            }
            
            # Add timestamp
            portfolio_data["last_updated"] = datetime.now().isoformat()
            
            return portfolio_data
        
        except Exception as e:
            logger.error(f"Error getting real portfolio data: {e}")
            logger.warning("Falling back to mock data")
            # Fall through to mock data
    
    # Return mock data for development/testing
    return {
        "portfolio": {
            "cash": 50000.0,
            "total_value": 100000.0,
            "positions": {
                "AAPL": {
                    "quantity": 100,
                    "avg_price": 150.0,
                    "current_price": 170.25,
                    "current_value": 17025.0,
                    "unrealized_pnl": 2025.0,
                    "unrealized_pnl_pct": 13.5
                },
                "MSFT": {
                    "quantity": 50,
                    "avg_price": 250.0,
                    "current_price": 280.50,
                    "current_value": 14025.0,
                    "unrealized_pnl": 1525.0,
                    "unrealized_pnl_pct": 6.1
                },
                "GOOGL": {
                    "quantity": 30,
                    "avg_price": 125.0,
                    "current_price": 135.75,
                    "current_value": 4072.5,
                    "unrealized_pnl": 322.5,
                    "unrealized_pnl_pct": 8.6
                }
            },
            "asset_allocation": {
                "Technology": 75.5,
                "Cash": 24.5
            }
        },
        "performance_metrics": {
            "cumulative_return": 15.2,
            "sharpe_ratio": 1.8,
            "max_drawdown": -8.5,
            "volatility": 12.3,
            "win_rate": 68.5,
            "profit_factor": 2.3,
            "recent_daily_returns": [0.8, -0.3, 1.2, 0.5, -0.2]
        },
        "recent_activity": {
            "trades": [
                {
                    "timestamp": datetime.now().isoformat(),
                    "symbol": "AAPL",
                    "action": "BUY",
                    "quantity": 25,
                    "price": 168.75
                },
                {
                    "timestamp": (datetime.now() - timedelta(hours=2)).isoformat(),
                    "symbol": "NFLX",
                    "action": "SELL",
                    "quantity": 15,
                    "price": 410.25
                }
            ],
            "signals": [
                {
                    "timestamp": datetime.now().isoformat(),
                    "symbol": "TSLA",
                    "signal_type": "BUY",
                    "strength": 0.85,
                    "source": "pattern_recognition"
                },
                {
                    "timestamp": (datetime.now() - timedelta(hours=1)).isoformat(),
                    "symbol": "MSFT",
                    "signal_type": "HOLD",
                    "strength": 0.62,
                    "source": "fundamental"
                }
            ]
        },
        "strategy_data": {
            "active_strategies": ["momentum", "mean_reversion", "trend_following"],
            "strategy_allocations": {
                "momentum": 40,
                "mean_reversion": 30,
                "trend_following": 30
            },
            "strategy_performance": {
                "momentum": {
                    "return": 12.5,
                    "sharpe": 1.4
                },
                "mean_reversion": {
                    "return": 8.2,
                    "sharpe": 1.1
                },
                "trend_following": {
                    "return": 15.8,
                    "sharpe": 1.7
                }
            }
        },
        "system_status": {
            "is_market_open": True,
            "market_hours": "9:30 AM - 4:00 PM ET",
            "data_providers": ["alpha_vantage", "yahoo_finance"],
            "connected_brokers": ["paper_trading"],
            "system_health": {
                "cpu_usage": 35,
                "memory_usage": 42,
                "disk_space": 75
            }
        },
        "learning_status": {
            "training_in_progress": False,
            "models_status": {
                "price_predictor": {
                    "accuracy": 0.72,
                    "last_trained": datetime.now().isoformat()
                },
                "volatility_estimator": {
                    "accuracy": 0.68,
                    "last_trained": datetime.now().isoformat()
                }
            },
            "recent_learning_metrics": {
                "training_cycles": 15,
                "validation_loss": 0.082,
                "training_time_seconds": 450
            }
        },
        "market_regime": "BULLISH_TREND",
        "last_updated": datetime.now().isoformat()
    }

# Helper function to execute trades (real or simulated)
def execute_trade(symbol, quantity, action, order_type="market", price=None):
    """Execute a trade either through real broker or simulation"""
    if USING_REAL_COMPONENTS:
        # In real implementation, this would connect to a broker API
        logger.info(f"Trade execution: {action} {quantity} {symbol}")
        # This would be a call to your trade executor component
        return {"status": "submitted", "order_id": "sim_" + datetime.now().strftime("%Y%m%d%H%M%S")}
    else:
        # Simulated trade execution
        logger.info(f"SIMULATION: {action} {quantity} {symbol}")
        return {"status": "filled", "order_id": "sim_" + datetime.now().strftime("%Y%m%d%H%M%S")}

# Helper function to get historical data for charts
def get_historical_data(symbol, days=30):
    """Get historical data for charting"""
    if USING_REAL_COMPONENTS and 'data_provider' in globals():
        # Use real data provider to get historical data
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        try:
            data = data_provider.get_historical_data([symbol], start_date, end_date)
            if symbol in data and not data[symbol].empty:
                return data[symbol]
        except Exception as e:
            logger.error(f"Error getting historical data: {e}")
    
    # Fall back to mock data
    dates = pd.date_range(end=datetime.now(), periods=days)
    base_price = 100 + np.random.rand() * 200  # Random starting price
    prices = [base_price]
    for _ in range(1, days):
        # Random walk with drift
        change = np.random.normal(0.0005, 0.015)  # Mean small positive drift
        prices.append(prices[-1] * (1 + change))
    
    # Create DataFrame with OHLCV data
    df = pd.DataFrame({
        'date': dates,
        'open': prices,
        'high': [p * (1 + np.random.uniform(0, 0.02)) for p in prices],
        'low': [p * (1 - np.random.uniform(0, 0.02)) for p in prices],
        'close': [p * (1 + np.random.normal(0, 0.005)) for p in prices],
        'volume': [int(np.random.uniform(100000, 10000000)) for _ in range(days)]
    })
    
    return df

# Helper function to convert time range to time parameters for data fetching
def convert_time_range(time_range):
    """Convert time range selection to days, hours, minutes and timeframe"""
    # Default values
    days = 7
    resolution = 'day'  # 'day', 'hour', 'minute'
    
    # Time range mappings
    if time_range == "1 Minute":
        days = 1/24/60  # 1 minute as fraction of a day
        resolution = 'minute'
    elif time_range == "5 Minutes":
        days = 5/24/60  # 5 minutes as fraction of a day
        resolution = 'minute'
    elif time_range == "15 Minutes":
        days = 15/24/60  # 15 minutes as fraction of a day
        resolution = 'minute'
    elif time_range == "30 Minutes":
        days = 30/24/60  # 30 minutes as fraction of a day
        resolution = 'minute'
    elif time_range == "1 Hour":
        days = 1/24  # 1 hour as fraction of a day
        resolution = 'hour'
    elif time_range == "8 Hours":
        days = 8/24  # 8 hours as fraction of a day
        resolution = 'hour'
    elif time_range == "1 Day":
        days = 1
        resolution = 'day'
    elif time_range == "7 Days":
        days = 7
        resolution = 'day'
    elif time_range == "1 Month":
        days = 30
        resolution = 'day'
    elif time_range == "3 Months":
        days = 90
        resolution = 'day'
    elif time_range == "6 Months":
        days = 180
        resolution = 'day'
    elif time_range == "1 Year":
        days = 365
        resolution = 'day'
    elif time_range == "All Time":
        days = 1000  # A large number to get all available data
        resolution = 'day'
    elif time_range == "Custom":
        # Custom will be handled by the date input fields
        days = 30  # Default for custom if not specified
        resolution = 'day'
    
    # For debugging
    logger.info(f"Converting time range '{time_range}' to {days} days with {resolution} resolution")
    
    return {
        'days': days,
        'resolution': resolution,
        'start_date': (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S'),
        'end_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

# Main dashboard content
st.markdown('<div class="main-header">Trading Strategy Dashboard</div>', unsafe_allow_html=True)

# Time Range Selector (moved from sidebar)
st.markdown('<div class="sub-header">Time Range</div>', unsafe_allow_html=True)
time_range = st.selectbox(
    "Select time period",
    [
        "1 Minute", "5 Minutes", "15 Minutes", "30 Minutes",
        "1 Hour", "8 Hours",
        "1 Day", "7 Days",
        "1 Month", "3 Months", "6 Months", "1 Year",
        "All Time", "Custom"
    ]
)
if time_range == "Custom":
    start_date = st.date_input("Start date", datetime.now() - timedelta(days=30))
    end_date = st.date_input("End date", datetime.now())

# Create top row with summary metrics
metric_cols = st.columns(4)

# Calculate filtered metrics based on selected strategies
win_rates = {"Momentum": 67.8, "Mean Reversion": 68.5, "Breakout": 62.3, "Volatility": 59.8, "Trend Following": 74.2, "Pairs Trading": 65.1}
profit_factors = {"Momentum": 1.92, "Mean Reversion": 1.78, "Breakout": 1.65, "Volatility": 1.59, "Trend Following": 2.15, "Pairs Trading": 1.72}
drawdowns = {"Momentum": -4.2, "Mean Reversion": -5.1, "Breakout": -6.8, "Volatility": -8.3, "Trend Following": -3.5, "Pairs Trading": -4.9}
sharpes = {"Momentum": 1.85, "Mean Reversion": 1.73, "Breakout": 1.62, "Volatility": 1.51, "Trend Following": 2.05, "Pairs Trading": 1.68}

# Calculate average metrics for selected strategies
if selected_strategies:
    avg_win_rate = sum(win_rates.get(s, 0) for s in selected_strategies) / len(selected_strategies)
    avg_profit_factor = sum(profit_factors.get(s, 0) for s in selected_strategies) / len(selected_strategies)
    avg_drawdown = sum(drawdowns.get(s, 0) for s in selected_strategies) / len(selected_strategies)
    avg_sharpe = sum(sharpes.get(s, 0) for s in selected_strategies) / len(selected_strategies)
    
    # Calculate deltas compared to all strategies
    all_avg_win_rate = sum(win_rates.values()) / len(win_rates)
    all_avg_profit_factor = sum(profit_factors.values()) / len(profit_factors)
    all_avg_drawdown = sum(drawdowns.values()) / len(drawdowns)
    all_avg_sharpe = sum(sharpes.values()) / len(sharpes)
    
    delta_win_rate = avg_win_rate - all_avg_win_rate
    delta_profit_factor = avg_profit_factor - all_avg_profit_factor
    delta_drawdown = avg_drawdown - all_avg_drawdown
    delta_sharpe = avg_sharpe - all_avg_sharpe
else:
    avg_win_rate = 67.8
    avg_profit_factor = 1.87
    avg_drawdown = -4.2
    avg_sharpe = 1.93
    delta_win_rate = 0
    delta_profit_factor = 0
    delta_drawdown = 0
    delta_sharpe = 0

with metric_cols[0]:
    st.metric(label="Total Win Rate", value=f"{avg_win_rate:.1f}%", delta=f"{delta_win_rate:.1f}%" if delta_win_rate != 0 else None)
with metric_cols[1]:
    st.metric(label="Profit Factor", value=f"{avg_profit_factor:.2f}", delta=f"{delta_profit_factor:.2f}" if delta_profit_factor != 0 else None)
with metric_cols[2]:
    st.metric(label="Max Drawdown", value=f"{avg_drawdown:.1f}%", delta=f"{delta_drawdown:.1f}%" if delta_drawdown != 0 else None, delta_color="inverse")
with metric_cols[3]:
    st.metric(label="Sharpe Ratio", value=f"{avg_sharpe:.2f}")

# Add live data section if available
if LIVE_DATA_MANAGER_AVAILABLE:
    live_data_manager.create_live_data_section(chart_library=go if PLOTLY_AVAILABLE else None)

# Create tabs for different sections
tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8, tab9, tab10, tab11, tab12 = st.tabs(["Dashboard", "Portfolio", "News/Predictions", "Strategy Optimizer", "Backtester", "Settings", "Developer", "LLM Insights", "AI Chat", "Modular Strategy", "Component Marketplace", "Trading Control"])

with tab1:
    # Portfolio Performance Chart (moved to top)
    st.markdown('<div class="sub-header">Portfolio Performance</div>', unsafe_allow_html=True)
    
    # Get time parameters from the time range selection
    time_params = convert_time_range(time_range)
    
    # Create appropriate sample data based on time range selection
    if time_params['resolution'] == 'minute':
        # Generate minute-level data
        date_range = pd.date_range(
            end=datetime.now(),
            periods=int(time_params['days'] * 24 * 60),  # Convert days to minutes
            freq='1min'
        )
        # Ensure date_range is not empty
        if len(date_range) == 0:
            date_range = pd.date_range(
                end=datetime.now(),
                periods=60,  # Default to last hour if calculation results in 0
                freq='1min'
            )
            
        base_value = 100000
        # Generate more noisy data for minute-level
        portfolio_values = [base_value]
        for i in range(1, len(date_range)):
            # Higher volatility for smaller timeframes
            change = np.random.normal(0.00001, 0.0002)  # Very small mean, higher variance
            portfolio_values.append(portfolio_values[-1] * (1 + change))
    elif time_params['resolution'] == 'hour':
        # Generate hourly data
        date_range = pd.date_range(
            end=datetime.now(),
            periods=int(time_params['days'] * 24),  # Convert days to hours
            freq='1H'
        )
        # Ensure date_range is not empty
        if len(date_range) == 0:
            date_range = pd.date_range(
                end=datetime.now(),
                periods=24,  # Default to last day if calculation results in 0
                freq='1H'
            )
            
        base_value = 100000
        portfolio_values = [base_value]
        for i in range(1, len(date_range)):
            change = np.random.normal(0.0001, 0.0005)  # Small mean, medium variance
            portfolio_values.append(portfolio_values[-1] * (1 + change))
    else:  # daily or longer
        # Generate daily data
        date_range = pd.date_range(
            end=datetime.now(),
            periods=int(time_params['days']),
            freq='1D'
        )
        # Ensure date_range is not empty
        if len(date_range) == 0:
            date_range = pd.date_range(
                end=datetime.now(),
                periods=7,  # Default to last week if calculation results in 0
                freq='1D'
            )
            
        base_value = 100000
        portfolio_values = [base_value]
        for i in range(1, len(date_range)):
            change = np.random.normal(0.0003, 0.001)  # Larger mean, lower relative variance
            portfolio_values.append(portfolio_values[-1] * (1 + change))
    
    # Create a dataframe with the sample data
    # Ensure all arrays have the same length
    portfolio_values = np.array(portfolio_values)
    
    # Make sure benchmark_values matches the length of portfolio_values
    benchmark_values = [base_value * (1 + 0.0001 * i + np.random.normal(0, 0.0005)) for i in range(len(portfolio_values))]
    
    # Ensure date_range has the same length as the other arrays
    if len(date_range) != len(portfolio_values):
        # Trim or extend date_range to match portfolio_values length
        if len(date_range) > len(portfolio_values):
            date_range = date_range[:len(portfolio_values)]
        elif len(date_range) > 0:  # Only try to extend if we have at least one date
            # If we need more dates, extend from the last date
            last_date = date_range[-1]
            freq = pd.infer_freq(date_range)
            extension = pd.date_range(
                start=last_date + pd.Timedelta(seconds=1), 
                periods=len(portfolio_values) - len(date_range),
                freq=freq or '1min'  # Use inferred frequency or default to 1min
            )
            date_range = pd.concat([date_range, extension])
        else:
            # If date_range is empty but we have portfolio values, create a new date range
            date_range = pd.date_range(
                end=datetime.now(),
                periods=len(portfolio_values),
                freq='1min'
            )
    
    # Double-check all arrays have the same length before creating DataFrame
    min_length = min(len(date_range), len(portfolio_values), len(benchmark_values))
    if min_length == 0:
        # Ensure we have at least some data
        min_length = 10
        date_range = pd.date_range(end=datetime.now(), periods=min_length, freq='1D')
        portfolio_values = np.array([base_value * (1 + 0.001 * i) for i in range(min_length)])
        benchmark_values = [base_value * (1 + 0.0005 * i) for i in range(min_length)]
    else:
        date_range = date_range[:min_length]
        portfolio_values = portfolio_values[:min_length]
        benchmark_values = benchmark_values[:min_length]
    
    portfolio_df = pd.DataFrame({
        'Date': date_range,
        'Portfolio Value': portfolio_values,
        'Benchmark': benchmark_values
    })
    
    # Display a chart with the data
    fig_portfolio = px.line(
        portfolio_df,
        x='Date',
        y=['Portfolio Value', 'Benchmark'],
        title=f"Portfolio vs Benchmark Performance ({time_range})",
        color_discrete_sequence=['#1E88E5', '#FFA000']
    )
    
    # Adjust the chart for different time resolutions
    if time_params['resolution'] == 'minute':
        fig_portfolio.update_xaxes(tickformat='%H:%M', tickmode='auto', nticks=10)
    elif time_params['resolution'] == 'hour':
        fig_portfolio.update_xaxes(tickformat='%m-%d %H:%M', tickmode='auto', nticks=8)
    else:
        fig_portfolio.update_xaxes(tickformat='%Y-%m-%d', tickmode='auto', nticks=10)
    
    st.plotly_chart(fig_portfolio, use_container_width=True)
    
    # Two column layout for allocation and trades
    alloc_col, trades_col = st.columns(2)
    
    with alloc_col:
        st.markdown('<div class="sub-header">🥧 Allocation Breakdown</div>', unsafe_allow_html=True)
        
        # Sample data for strategy allocation
        strategies = ["Momentum", "Mean Reversion", "Breakout", "Volatility", "Trend Following", "Pairs Trading"]
        allocations = [30, 25, 15, 10, 15, 5]
        
        # Filter strategies based on selection
        filtered_strategies = [s for s in strategies if s in selected_strategies]
        filtered_allocations = [allocations[strategies.index(s)] for s in filtered_strategies]
        
        # Normalize allocations after filtering
        if filtered_allocations:
            total_allocation = sum(filtered_allocations)
            filtered_allocations = [a * 100 / total_allocation for a in filtered_allocations]
        
        # Create allocation pie chart
        fig_allocation = px.pie(
            values=filtered_allocations,
            names=filtered_strategies,
            title="Current Capital Allocation",
            color_discrete_sequence=px.colors.qualitative.Bold
        )
        fig_allocation.update_traces(textposition='inside', textinfo='percent+label')
        st.plotly_chart(fig_allocation, use_container_width=True)
    
    with trades_col:
        st.markdown('<div class="sub-header">📋 Recent Trades</div>', unsafe_allow_html=True)
        
        # Sample trade data
        trades_data = {
            "Symbol": ["AAPL", "MSFT", "TSLA", "AMZN", "NVDA", "GOOGL", "QQQ", "SPY"],
            "Strategy": ["Momentum", "Breakout", "Volatility", "Mean Reversion", "Momentum", "Trend Following", "Pairs Trading", "Mean Reversion"],
            "Confidence": [87, 92, 65, 78, 94, 83, 71, 89],
            "Entry Date": ["2023-04-10", "2023-04-09", "2023-04-08", "2023-04-07", "2023-04-06", "2023-04-05", "2023-04-04", "2023-04-03"],
        }
        
        # AI predictions section
        st.markdown('<div class="sub-header">AI Market Predictions</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
try:
    from live_data_manager import LiveDataManager
    LIVE_DATA_MANAGER_AVAILABLE = True
except ImportError:
    LIVE_DATA_MANAGER_AVAILABLE = False
    logger.warning("LiveDataManager not available, live data features will be disabled")

# Add this at the beginning of the main code body, after all imports/initializations are done
# Initialize the LiveDataManager if available
if LIVE_DATA_MANAGER_AVAILABLE:
    live_data_manager = LiveDataManager()
    # Check for auto-refresh
    live_data_manager.check_auto_refresh()

# Add this right after the metrics section (after the code that displays the 4 main metrics)
# Add live data section if available
if LIVE_DATA_MANAGER_AVAILABLE:
    live_data_manager.create_live_data_section(chart_library=go if PLOTLY_AVAILABLE else None)

if news_data:
    with sentiment_cols[0]:
        for news in positive_news:
            article_url = news.get("article_url", news.get("url", "#"))
            clean_text = clean_summary(news.get("summary", ""))
            time_ago = format_timestamp(news.get("timestamp", ""))
            source = news.get('source', 'Unknown')
            
            # Generate trading impact
            impact_data = generate_trading_impact(news)
            
            # Get image URL or fallback
            image_url = news.get("image_url", "")
            if not image_url:
                image_url = get_fallback_image("Neutral", source)
            
            st.markdown(f"""
<div style="height: 450px; overflow: hidden; margin-bottom: 15px; padding: 0; border-radius: 5px; border: 1px solid #ddd; border-left: 5px solid #2E7D32; background-color: white; position: relative;">
<div style="height: 120px; overflow: hidden; background-color: #f5f5f5; display: flex; align-items: center; justify-content: center;">
<img src="{image_url}" style="max-width: 100%; max-height: 120px; object-fit: cover;" onerror="this.onerror=null; this.src='https://img.icons8.com/color/96/000000/up-trending.png';">
</div>
<div style="padding: 15px;">
<div style="font-weight: bold; margin-bottom: 8px; height: 40px; overflow: hidden;">
{news.get('title', 'No Title')}
</div>
<div style="color: #444; font-size: 14px; margin-bottom: 10px; line-height: 1.4; height: 60px; overflow: hidden;">
{clean_text[:150]}{"..." if len(clean_text) > 150 else ""}
</div>
<div style="font-size: 13px; color: #2E7D32; background-color: #E8F5E9; padding: 10px; border-radius: 4px; margin-bottom: 10px; height: 135px; overflow-y: auto;">
    <strong style="font-size: 14px;">STRATEGY: {impact_data.get('strategy', 'Bullish Position')}</strong><br>
    <strong>Why:</strong> {impact_data.get('rationale', 'Positive sentiment')}<br>
    <strong>How:</strong> {impact_data.get('implementation', 'Consider bullish positions')}<br>
    <strong>Timeframe:</strong> {impact_data.get('timeframe', 'Short to medium-term')}<br>
    <strong>Risk:</strong> {impact_data.get('risk_level', 'Moderate')}
</div>
<div style="display: flex; justify-content: space-between; font-size: 12px; color: #666; position: absolute; bottom: 15px; left: 15px; right: 15px;">
    <span>
        <strong>{source}</strong> &bull; {time_ago}
    </span>
    <a href="{article_url}" target="_blank" style="color: #1E88E5; text-decoration: none; font-weight: bold;">
        More
    </a>
</div>
</div>
</div>
""", unsafe_allow_html=True)
                
    with sentiment_cols[2]:
        for news in negative_news:
            article_url = news.get("article_url", news.get("url", "#"))
            clean_text = clean_summary(news.get("summary", ""))
            time_ago = format_timestamp(news.get("timestamp", ""))
            source = news.get('source', 'Unknown')
            
            # Generate trading impact
            impact_data = generate_trading_impact(news)
            
            # Get image URL or fallback
            image_url = news.get("image_url", "")
            if not image_url:
                image_url = get_fallback_image("Negative", source)
            
            st.markdown(f"""
<div style="height: 450px; overflow: hidden; margin-bottom: 15px; padding: 0; border-radius: 5px; border: 1px solid #ddd; border-left: 5px solid #C62828; background-color: white; position: relative;">
    <div style="height: 120px; overflow: hidden; background-color: #f5f5f5; display: flex; align-items: center; justify-content: center;">
        <img src="{image_url}" style="max-width: 100%; max-height: 120px; object-fit: cover;" onerror="this.onerror=null; this.src='https://img.icons8.com/color/96/000000/down-trending.png';">
    </div>
    <div style="padding: 15px;">
        <div style="font-weight: bold; margin-bottom: 8px; height: 40px; overflow: hidden;">
            {news.get('title', 'No Title')}
        </div>
        <div style="color: #444; font-size: 14px; margin-bottom: 10px; line-height: 1.4; height: 60px; overflow: hidden;">
            {clean_text[:150]}{"..." if len(clean_text) > 150 else ""}
        </div>
        <div style="font-size: 13px; color: #C62828; background-color: #FFEBEE; padding: 10px; border-radius: 4px; margin-bottom: 10px; height: 135px; overflow-y: auto;">
            <strong style="font-size: 14px;">STRATEGY: {impact_data.get('strategy', 'Defensive Position')}</strong><br>
            <strong>Why:</strong> {impact_data.get('rationale', 'Negative sentiment')}<br>
            <strong>How:</strong> {impact_data.get('implementation', 'Consider hedging or reducing exposure')}<br>
            <strong>Timeframe:</strong> {impact_data.get('timeframe', 'Short to medium-term')}<br>
            <strong>Risk:</strong> {impact_data.get('risk_level', 'Moderate to High')}
        </div>
        <div style="display: flex; justify-content: space-between; font-size: 12px; color: #666; position: absolute; bottom: 15px; left: 15px; right: 15px;">
            <span>
                <strong>{source}</strong> &bull; {time_ago}
            </span>
            <a href="{article_url}" target="_blank" style="color: #1E88E5; text-decoration: none; font-weight: bold;">
                More
            </a>
        </div>
    </div>
</div>
""", unsafe_allow_html=True)
else:
    # Search prompt if no results
    st.markdown("""
    <div class="metric-card">
        <p>Use the search box above to find news about any company or ticker symbol.</p>
        <p>For example, try searching for \"AAPL\", \"Microsoft\", \"Tesla\", or \"Cryptocurrency\".</p>
    </div>
    """, unsafe_allow_html=True)
with col1:
    st.markdown("### Market Regime Forecast")
    
    # Try to get real market regime forecasts using the ML classifier
    # Try to get real market regime forecasts using the ML classifier
